Directory structure:
â””â”€â”€ marcelo-earth-generative-manim/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CODE_OF_CONDUCT.md
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ docs.py
    â”œâ”€â”€ GenScene.py
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ main.py
    â”œâ”€â”€ packages.txt
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ run.py
    â”œâ”€â”€ .editorconfig
    â”œâ”€â”€ animo/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â”œâ”€â”€ examples/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ generate_video.py
    â”‚   â”‚   â”œâ”€â”€ quick_generate.py
    â”‚   â”‚   â””â”€â”€ sample.py
    â”‚   â””â”€â”€ src/
    â”‚       â””â”€â”€ animo/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ client.py
    â”‚           â””â”€â”€ resources/
    â”‚               â”œâ”€â”€ __init__.py
    â”‚               â””â”€â”€ videos.py
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ run.py
    â”‚   â”œâ”€â”€ .env.example
    â”‚   â”œâ”€â”€ prompts/
    â”‚   â”‚   â””â”€â”€ manimDocs.py
    â”‚   â”œâ”€â”€ public/
    â”‚   â”‚   â””â”€â”€ openapi.yaml
    â”‚   â””â”€â”€ routes/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ chat_generation.py
    â”‚       â”œâ”€â”€ code_generation.py
    â”‚       â””â”€â”€ video_rendering.py
    â”œâ”€â”€ datasets/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ physics-01.jsonl
    â”‚   â”œâ”€â”€ code/
    â”‚   â”‚   â”œâ”€â”€ ArgMinExample.md
    â”‚   â”‚   â”œâ”€â”€ ArgMinExample.py
    â”‚   â”‚   â”œâ”€â”€ BooleanOperations.py
    â”‚   â”‚   â”œâ”€â”€ BraceAnnotation.py
    â”‚   â”‚   â”œâ”€â”€ FixedInFrameMObjectTest.py
    â”‚   â”‚   â”œâ”€â”€ FollowingGraphCamera.py
    â”‚   â”‚   â”œâ”€â”€ GradientImageFromArray.py
    â”‚   â”‚   â”œâ”€â”€ GraphAreaPlot.py
    â”‚   â”‚   â”œâ”€â”€ HeatDiagramPlot.py
    â”‚   â”‚   â”œâ”€â”€ ManimCELogo.py
    â”‚   â”‚   â”œâ”€â”€ MovingAngle.py
    â”‚   â”‚   â”œâ”€â”€ MovingAround.py
    â”‚   â”‚   â”œâ”€â”€ MovingDots.py
    â”‚   â”‚   â”œâ”€â”€ MovingFrameBox.py
    â”‚   â”‚   â”œâ”€â”€ MovingGroupToDestination.py
    â”‚   â”‚   â”œâ”€â”€ MovingZoomedSceneAround.py
    â”‚   â”‚   â”œâ”€â”€ OpeningManim.py
    â”‚   â”‚   â”œâ”€â”€ PointMovingOnShapes.py
    â”‚   â”‚   â”œâ”€â”€ PointWithTrace.py
    â”‚   â”‚   â”œâ”€â”€ PolygonOnAxes.py
    â”‚   â”‚   â”œâ”€â”€ RotationUpdater.py
    â”‚   â”‚   â”œâ”€â”€ SinAndCosFunctionPlot.py
    â”‚   â”‚   â”œâ”€â”€ SineCurveUnitCircle.py
    â”‚   â”‚   â”œâ”€â”€ ThreeDCameraIllusionRotation.py
    â”‚   â”‚   â”œâ”€â”€ ThreeDCameraRotation.py
    â”‚   â”‚   â”œâ”€â”€ ThreeDLightSourcePosition.py
    â”‚   â”‚   â”œâ”€â”€ ThreeDSurfacePlot.py
    â”‚   â”‚   â””â”€â”€ VectorArrow.py
    â”‚   â”œâ”€â”€ extract/
    â”‚   â”‚   â””â”€â”€ edoh_dataset.py
    â”‚   â”œâ”€â”€ format/
    â”‚   â”‚   â””â”€â”€ formatter_edoh.py
    â”‚   â””â”€â”€ prompt/
    â”‚       â”œâ”€â”€ API_Key.cfg
    â”‚       â”œâ”€â”€ prompt_from_OpenAIGPT.py
    â”‚       â””â”€â”€ prompt_from_QianFan.py
    â”œâ”€â”€ experiments/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ generative_manim_claude_sonnet.py
    â”‚   â”œâ”€â”€ Generative_Manim_LangGraph_Implementation.ipynb
    â”‚   â”œâ”€â”€ generative_manim_langgraph_implementation.py
    â”‚   â””â”€â”€ langgraph_minimal_implementation.py
    â”œâ”€â”€ streamlit/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ docker-compose.yml
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â””â”€â”€ src/
    â”‚       â”œâ”€â”€ main.py
    â”‚       â”œâ”€â”€ utils.py
    â”‚       â””â”€â”€ pages/
    â”‚           â”œâ”€â”€ 1_ğŸ’»_Render_Engine.py
    â”‚           â”œâ”€â”€ 2_ğŸ¤–_Prompt_Engine.py
    â”‚           â”œâ”€â”€ 3_âš¡ï¸_How_it_works.py
    â”‚           â””â”€â”€ 4_â“_FAQ.py
    â””â”€â”€ .devcontainer/
        â””â”€â”€ devcontainer.json

================================================
FILE: README.md
================================================
<p align="center">
  <img
    src=".github/logo.png"
    align="center"
    width="100"
    alt="Generative Manim"
    title="Generative Manim"
  />
  <h1 align="center">Generative Manim</h1>
</p>

<p align="center">
  ğŸ¨ GPT-4o powered generative videos. Concept. âš¡ï¸ <a href="https://discord.gg/HkbYEGybGv">Join our Discord server here!</a>
</p>

<p align="center">
  <a href="https://generative-manim.vercel.app">
    <img src="https://img.shields.io/static/v1?label=Demo&message=Generative%20Manim&color=000000&logo=vercel&style=flat" />
  </a>
  <a href="https://animo.video">
    <img src="https://img.shields.io/static/v1?label=Platform&message=Animo&color=E11D48&logo=openai&style=flat" />
  </a>
  <a href="">
    <img src="https://img.shields.io/static/v1?label=OpenAI%20API&message=GPT-4o&color=000000&logo=openai&style=flat" />
  </a>
  <a href="">
    <img src="https://img.shields.io/static/v1?label=Anthropic&message=Claude&color=000000&logo=anthropic&style=flat" />
  </a>
</p>

---

![Preview](./.github/preview.jpg)

## ğŸš€ Concept

**Generative Manim** (GM) is a suite of tools that allows you to create videos with Manim using LLMs (Large Language Models) like GPT-4 or Claude. The idea is to enable anyone to create wonderful animations from text âœ¨.

It began as a prototype of a web app that uses [GPT-4](https://openai.com/research/gpt-4) to generate videos with [Manim](https://www.manim.community). The idea behind this project is taking advantage of the power of LLMs in programming, the understanding of human language and the animation capabilities of Manim to generate a tool that could be used by anyone to create videos. Regardless of their programming or video editing skills.

- ğŸ–ï¸ [Generative Manim Demo](https://generative-manim.vercel.app/): Check out the demo of Generative Manim!
- ğŸ”¬ [Generative Manim API](https://github.com/360macky/generative-manim/tree/main/api): Build over the Animation Processing Interface, or API.
- ğŸ§‘â€ğŸ’» [Generative Manim Developers](https://discord.gg/HkbYEGybGv): Join our Discord server, learn new things, share your creations and more!
- ğŸ [Generative Manim Streamlit (Legacy)](https://github.com/360macky/generative-manim/tree/main/streamlit): First LLM exploration of LLMs and Animation.

## ğŸ’» Models

**Models** are the core of Generative Manim. A model is a way to convert text to code, that can later be rendered in a video.

| Name                          | Description                                                               | Engine                     | Phase |
| ----------------------------- | ------------------------------------------------------------------------- | -------------------------- | ----- |
| GM GPT-4o                     | Latest GPT model from OpenAI powered by a custom System Prompt            | GPT-4o                     | âœ…    |
| GM GPT-3.5 Fine Tuned         | First Fine-tuned model of GPT-3.5                                         | GPT-3.5                    | âœ…    |
| GM GPT-3.5 Physics Fine Tuned | Fine-tuned GPT-3.5 model trained to generate Physics animations           | GPT-3.5                    | âœ…    |
| GM Claude Sonnet              | Claude Sonnet 3 model from Sonnet adapted with our custom System Prompt   | claude-3-sonnet-20240229   | âœ…    |
| GM Claude Sonnet 3.5          | Claude Sonnet 3.5 model from Sonnet adapted with our custom System Prompt | claude-3-5-sonnet-20240620 | âœ…    |

### ğŸ“¡ New Models

If you want to suggest a new model, please open an issue in the [repository](https://github.com/360macky/generative-manim/issues) or talk with us in our [Discord server](https://discord.gg/HkbYEGybGv).

## âœ¨ Sponsors

**Generative Manim** is currently sponsored by **The Astronomical Software Company**.

## ğŸ¤² Contributing

Generative Manim is an open source project.

If you want to be the author of a new feature, fix a bug or contribute with something new.

Fork the repository and make changes as you like. [Pull requests](https://github.com/360macky/generative-manim/pulls) are warmly welcome. Remember you can also join our [Discord server](https://discord.gg/HkbYEGybGv) to discuss new features, bugs or any other topic.



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
hello@marceloarias.com.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.



================================================
FILE: Dockerfile
================================================
# Use an official lightweight Python image.
FROM python:3.9-slim

# Set the working directory to /app
WORKDIR /app

# Preparing Manim dependencies and build tools
RUN apt-get update && apt-get -y upgrade && \
    apt-get install -y build-essential libcairo2-dev libpango1.0-dev pkg-config ffmpeg curl \
    texlive texlive-latex-extra texlive-fonts-extra texlive-latex-recommended texlive-science

# Copy the current directory contents into the container at /app
COPY . /app

# Set the working directory to /app/api, where your Flask app is
WORKDIR /app/api

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Make port 8080 available to the world outside this container
EXPOSE 8080

# Run run.py when the container launches
CMD ["python", "../run.py"]



================================================
FILE: docs.py
================================================
import os
import re
import time
import requests
import html2text
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

# Base URL of the documentation
BASE_URL = "https://docs.manim.community/en/stable/"

# Base directory to save the markdown files
OUTPUT_DIR = "docs_md"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# A session to reuse HTTP connections
session = requests.Session()

def is_valid_url(url):
    """
    Only allow URLs that belong to the docs.manim.community/en/stable/ site.
    """
    parsed = urlparse(url)
    base_parsed = urlparse(BASE_URL)
    return (parsed.scheme in ("http", "https") and 
            parsed.netloc == base_parsed.netloc and 
            parsed.path.startswith(base_parsed.path))

def url_to_local_path(url):
    """
    Convert a URL into a local file path that preserves the URLâ€™s folder structure.
    
    For example, a URL ending with:
        /_modules/manim/mobject/geometry/line.html
    will be saved as:
        docs_md/_modules/manim/mobject/geometry/line.html.md
    """
    parsed = urlparse(url)
    base_path = urlparse(BASE_URL).path
    # Get the relative path after the base
    rel_path = parsed.path[len(base_path):].lstrip("/")
    if not rel_path:
        rel_path = "index.html"
    local_path = os.path.join(OUTPUT_DIR, rel_path)
    # Ensure the file ends with .md (appending .md even if it ends with .html)
    local_path += ".md"
    return local_path

def convert_html_to_markdown(html_content):
    """
    Convert HTML content to Markdown using html2text.
    """
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.body_width = 0  # do not wrap lines
    return h.handle(html_content)

def crawl(url, visited):
    """
    Recursively crawl the documentation pages starting from the given URL.
    """
    if url in visited:
        return
    print(f"Processing: {url}")
    visited.add(url)
    
    try:
        response = session.get(url)
        response.raise_for_status()
    except Exception as e:
        print(f"Failed to get {url}: {e}")
        return

    html_content = response.text
    soup = BeautifulSoup(html_content, "html.parser")
    
    # Extract only the first element with class "content"
    content_div = soup.find(class_="content")
    if content_div:
        content_html = str(content_div)
    else:
        print(f"No content div found in {url}; using full page.")
        content_html = html_content

    markdown = convert_html_to_markdown(content_html)
    
    # Determine the local file path and ensure its directory exists
    local_path = url_to_local_path(url)
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    
    with open(local_path, "w", encoding="utf-8") as f:
        f.write(markdown)
    print(f"Saved markdown to {local_path}")

    # Find and process links on the page
    for link in soup.find_all("a", href=True):
        href = link["href"]
        full_url = urljoin(url, href)
        full_url = full_url.split("#")[0]  # remove any fragment identifier
        if is_valid_url(full_url) and full_url not in visited:
            time.sleep(0.1)  # be polite with a short delay
            crawl(full_url, visited)

def combine_markdown_files(root_dir, output_file):
    """
    Recursively traverse root_dir and combine all .md files into one huge Markdown file.
    A heading structure (with '#' characters) is added based on the folder hierarchy.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        def process_dir(current_dir, level):
            # Write a heading for the current directory (skip if we're at the root)
            if os.path.abspath(current_dir) != os.path.abspath(root_dir):
                dir_name = os.path.basename(current_dir)
                out.write("\n" + "#" * level + " " + dir_name + "\n\n")
            
            # Get sorted list of items
            items = sorted(os.listdir(current_dir))
            # Separate directories and markdown files
            dirs = [i for i in items if os.path.isdir(os.path.join(current_dir, i))]
            md_files = [i for i in items if os.path.isfile(os.path.join(current_dir, i)) and i.endswith(".md")]
            
            # Process markdown files in the current directory
            for md_file in md_files:
                file_path = os.path.join(current_dir, md_file)
                # Use a heading level one deeper than the directory
                out.write("\n" + "#" * (level + 1) + " " + md_file + "\n\n")
                with open(file_path, "r", encoding="utf-8") as f:
                    out.write(f.read() + "\n\n")
            
            # Recursively process subdirectories
            for d in dirs:
                process_dir(os.path.join(current_dir, d), level + 1)
                
        process_dir(root_dir, 1)
    print(f"Combined markdown saved to {output_file}")

if __name__ == "__main__":
    visited = set()
    crawl(BASE_URL, visited)
    print("Download complete.")

    # After crawling, combine all markdown files into one huge markdown file.
    combined_output = "combined_docs.md"
    combine_markdown_files(OUTPUT_DIR, combined_output)



================================================
FILE: GenScene.py
================================================
# Manim code generated with OpenAI GPT
# Command to generate animation: manim GenScene.py GenScene --format=mp4 --media_dir . --custom_folders video_dir

from manim import *
from math import *

class GenScene(Scene):
    def construct(self):
        pass



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: main.py
================================================
import os
import streamlit as st
from PIL import Image

st.set_page_config(
    page_title="Generative Manim",
)

styl = f"""
<style>
  textarea[aria-label="Code generated: "] {{
    font-family: 'Consolas', monospace !important;
  }}
  }}
</style>
"""
st.markdown(styl, unsafe_allow_html=True)

st.title(":art: Generative Manim")
st.write(":robot_face: Create beautiful and quick animations with GPT-4 and GPT-3.5 :sparkles:")

st.write("## Hello! The new demo it's on [GM Demo](https://generative-manim.vercel.app) :rocket:")

st.write('Made with :heart: by [Marcelo](https://github.com/360macky).')
st.write('[Source code](https://github.com/360macky/generative-manim) - [Report a bug](https://github.com/360macky/generative-manim/issues/new) - [Twitter](https://twitter.com/360macky) - [OpenAI Profile](https://community.openai.com/u/360macky/summary)')



================================================
FILE: packages.txt
================================================
pkg-config
libglib2.0-dev
libcairo2-dev
libpango1.0-dev
libffi-dev
ffmpeg
texlive-full



================================================
FILE: pyproject.toml
================================================
[tool.poetry]
name = "streamlit-manim"
version = "0.1.0"
description = ""
authors = ["ash2shukla"]

[tool.poetry.dependencies]
python = ">=3.9.8,<3.11"
manim = "^0.15.1"
streamlit = "^1.14.0"
protobuf = "^3.20.0"
openai = "^0.27.2"

[tool.poetry.dev-dependencies]

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"




================================================
FILE: run.py
================================================
"""
The following script is used to run the API server located in the `api` directory.
"""

import os
from api import create_app

app = create_app()

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(debug=False, host="0.0.0.0", port=port)



================================================
FILE: .editorconfig
================================================
tab_width = 2
charset = utf-8



================================================
FILE: animo/README.md
================================================
<p align="center">
  <img
    src="https://raw.githubusercontent.com/marcelo-earth/generative-manim/main/.github/logo.png"
    align="center"
    width="100"
    alt="Animo"
    title="Animo"
  />
  <h1 align="center">Animo</h1>
</p>

<p align="center">
  ğŸ¨ Create animations from text using Manim under the hood âœ¨
</p>

<p align="center">
  <a href="https://animo.video">
    <img src="https://img.shields.io/static/v1?label=Platform&message=Animo&color=E11D48&logo=openai&style=flat" />
  </a>
  <a href="https://github.com/marcelo-earth/generative-manim">
    <img src="https://img.shields.io/static/v1?label=GitHub&message=Repository&color=181717&logo=github&style=flat" />
  </a>
  <a href="https://discord.com/invite/HkbYEGybGv">
    <img src="https://img.shields.io/static/v1?label=Discord&message=Community&color=5865F2&logo=discord&style=flat" />
  </a>
  <a href="https://pypi.org/project/animo/">
    <img src="https://img.shields.io/pypi/v/animo.svg?color=blue&logo=python&logoColor=white" />
  </a>
</p>

---

## ğŸš€ What is Animo?

Animo is a Python package that allows you to create animations from text using Manim under the hood. It's part of the [Generative Manim](https://github.com/marcelo-earth/generative-manim) suite of tools that enables anyone to create wonderful animations from text.

Visit [animo.video](https://animo.video) to learn more about the platform and see examples of what you can create.

## ğŸ“¦ Installation

```bash
pip install animo
```

## ğŸ”‘ API Key

To use Animo, you'll need an API key:

1. Visit [https://animo.video/account/developer](https://animo.video/account/developer)
2. Click on "Generate secret key"
3. Copy your API key and use it in your code

You'll also find quickstart guides and additional documentation on the developer page to help you get started.

## ğŸ”§ Usage

### Basic Usage

```python
from animo import Animo

client = Animo(api_key="your_api_key")

# Create a single video
code = """
class GenScene(Scene):
    def construct(self):
        circle = Circle(color=BLUE)
        self.play(Create(circle))
"""

response = client.videos.create(
    code=code,
    file_class="GenScene",
    aspect_ratio="16:9"
)

# Response format
response = {
    "message": "Video generation completed",
    "video_url": "https://animovideo.blob.core.windows.net/animocontainer/video-xxx.mp4"
}

# Export multiple scenes
scenes = [
    {"videoUrl": "https://animovideo.blob.core.windows.net/animocontainer/scene1.mp4"},
    {"videoUrl": "https://animovideo.blob.core.windows.net/animocontainer/scene2.mp4"}
]

export_response = client.videos.export(
    scenes=scenes,
    title_slug="my-animation"
)
```

### Generating Videos from Text Prompts

```python
from animo import Animo
import time
import sys

# Initialize client
client = Animo(api_key="your_api_key")

# Define your prompt
prompt = "Create a blue square"

print(f"ğŸš€ Generating video for: '{prompt}'")

# Start generation
try:
    generation = client.videos.generate(prompt=prompt)
    request_id = generation.get("requestId")
    
    if not request_id:
        print("âŒ No request ID received")
        sys.exit(1)
        
    print(f"âœ… Generation started with ID: {request_id}")
    
    # Poll for status with a simple progress indicator
    print("â³ Waiting for completion", end="")
    
    while True:
        status_data = client.videos.retrieve(request_id=request_id)
        status = status_data.get("status")
        
        # Update progress indicator
        sys.stdout.write(".")
        sys.stdout.flush()
        
        # Check for completion or error
        if status == "SUCCEEDED":
            video_url = status_data.get("videoUrl")
            print(f"\n\nğŸ¬ Video ready! URL: {video_url}")
            
            # Print processing time if available
            if processing_time := status_data.get("processingTime"):
                print(f"â±ï¸  Processing time: {processing_time} seconds")
                
            break
            
        elif status == "FAILED":
            error = status_data.get("error") or "Unknown error"
            print(f"\n\nâŒ Generation failed: {error}")
            break
            
        # Wait before next check
        time.sleep(3)
        
except Exception as e:
    print(f"\n\nâŒ Error: {str(e)}")
```

## ğŸ¤ Contributing

We welcome contributions to Animo! Here's how to get started:

### Requirements
- Python 3.9.6 or later
- Poetry (recommended for dependency management)

### Setup for Development

1. Clone the repository
```bash
git clone https://github.com/marcelo-earth/generative-manim.git
cd generative-manim/animo
```

2. Install dependencies
```bash
poetry install
```

### Community

Join our [Discord community](https://discord.com/invite/HkbYEGybGv) to connect with other Animo users, share your creations, and get help.

## ğŸ“„ License

[MIT License](LICENSE)



================================================
FILE: animo/pyproject.toml
================================================
[tool.poetry]
name = "animo"
version = "0.1.4"
description = "Create animations from text using Manim"
authors = ["Marcelo <team@animo.video>"]
readme = "README.md"
packages = [{include = "animo", from = "src"}]
license = "MIT"
homepage = "https://animo.video"
repository = "https://github.com/marcelo-earth/generative-manim"
keywords = ["manim", "animation", "video", "generative"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]

[tool.poetry.dependencies]
python = ">=3.9.6"
requests = "^2.31.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api" 


================================================
FILE: animo/examples/README.md
================================================
# Animo API Examples

This directory contains example scripts demonstrating how to use the Animo API for creating animations with Manim.

## Setup

Before running any examples, make sure to:

1. Install the Animo package:
   ```
   pip install animo
   ```

2. Set your API key as an environment variable:
   ```
   export ANIMO_API_KEY="your_api_key_here"
   ```

## Available Examples

### 1. Basic Manim Code Rendering

**File:** `sample.py`

This example demonstrates how to render a video from Manim code using the Animo API. It creates a simple animation that writes "Arte de programar" on the screen.

```
python sample.py
```

### 2. AI-Generated Video with Polling

**File:** `generate_video.py`

This example shows how to generate a video from a text prompt using AI. It includes detailed status polling and displays the generated Manim code.

```
python generate_video.py
```

### 3. Quick Video Generation Test

**File:** `quick_generate.py`

A streamlined script for testing the video generation API with minimal output. You can provide your own prompt as a command-line argument:

```
python quick_generate.py "Create an animation of a bouncing ball with physics"
```

Or run without arguments to use the default prompt:

```
python quick_generate.py
```

## API Response Structure

The video generation API returns responses with the following structure:

- Initial generation response:
  ```json
  {
    "message": "Your request has been submitted and is being processed.",
    "requestId": "210b186f-c50a-4f40-bd4c-bb73c154afec",
    "status": "PENDING"
  }
  ```

- Status check response:
  ```json
  {
    "createdAt": "2025-03-16T15:34:29.006000+00:00",
    "error": null,
    "generatedCode": "from manim import *\n\nclass GenScene(Scene):\n    def construct(self):\n        # Animation code here",
    "processingTime": 45.2,
    "requestId": "210b186f-c50a-4f40-bd4c-bb73c154afec",
    "status": "COMPLETED",
    "updatedAt": "2025-03-16T15:35:14.132000+00:00",
    "videoUrl": "https://storage.animo.video/videos/210b186f-c50a-4f40-bd4c-bb73c154afec.mp4"
  }
  ``` 


================================================
FILE: animo/examples/generate_video.py
================================================
from animo import Animo
import os
import time
from pprint import pprint

# Get API key from environment variable (recommended)
api_key = os.getenv("ANIMO_API_KEY")

# Initialize the Animo client
client = Animo(api_key=api_key)

# Define the prompt for generating a video
prompt = "Create a simple animation of a circle transforming into a square and use voiceover"

print("ğŸ¤– Generating video from prompt...")

try:
    # Generate the video
    response = client.videos.generate(
        prompt=prompt,
        engine="anthropic",
        model="claude-3-7-sonnet-20250219"
    )
    
    # Print the initial response
    print("\nğŸ“‹ Initial Response:")
    pprint(response)
    
    # Get the request ID
    request_id = response.get("requestId")
    
    if not request_id:
        print("\nâŒ Error: No request ID in the response")
        exit(1)
    
    print(f"\nâ³ Processing request ID: {request_id}")
    print("Checking status every 5 seconds...")
    
    # Poll for status until complete
    while True:
        # Wait 5 seconds between status checks
        time.sleep(5)
        
        # Check the status
        status_response = client.videos.retrieve(request_id=request_id)
        status = status_response.get("status")
        
        print(f"Status: {status}")
        
        # If there's an error, exit
        if status_response.get("error"):
            print(f"\nâŒ Error: {status_response.get('error')}")
            break
        
        # If complete, show the video URL
        if status == "COMPLETED":
            video_url = status_response.get("videoUrl")
            generated_code = status_response.get("generatedCode")
            
            print("\nâœ… Success! Your video is ready.")
            print(f"Video URL: {video_url}")
            
            if generated_code:
                print("\nğŸ“ Generated Manim Code:")
                print(generated_code)
            
            break
        
        # If failed, exit
        if status == "FAILED":
            print("\nâŒ Generation failed")
            break

except Exception as e:
    print(f"\nâŒ Error: {str(e)}") 


================================================
FILE: animo/examples/quick_generate.py
================================================
from animo import Animo
import os
import time
import sys
import platform
import subprocess
import json

# Get API key from environment variable
api_key = "Test123"
if not api_key:
    print("Error: ANIMO_API_KEY environment variable not set")
    sys.exit(1)

# Initialize client
client = Animo(api_key=api_key)

# Get prompt from command line or use default
prompt = sys.argv[1] if len(sys.argv) > 1 else "Create a heart shape, and explain the basic definition of love with voiceover"

print(f"ğŸš€ Generating video for: '{prompt}'")

# Start generation
try:
    generation = client.videos.generate(prompt=prompt)
    request_id = generation.get("requestId")
    
    if not request_id:
        print("âŒ No request ID received")
        sys.exit(1)
        
    print(f"âœ… Generation started with ID: {request_id}")
    
    # Poll for status with a simple progress indicator
    print("â³ Waiting for completion")
    last_status = None
    
    while True:
        status_data = client.videos.retrieve(request_id=request_id)
        status = status_data.get("status")
        
        # Print full response data for debugging
        print("\nğŸ“Š Full response data:")
        print(json.dumps(status_data, indent=2))
        print()
        
        # Only print status update when it changes
        if status != last_status:
            timestamp = status_data.get("updatedAt", "")
            timestamp_str = f" (updated: {timestamp})" if timestamp else ""
            
            if status == "PENDING":
                print(f"â³ Request pending{timestamp_str}")
            elif status == "PROCESSING":
                print(f"ğŸ”„ Processing video{timestamp_str}")
            elif status == "RENDERED":
                print(f"ğŸï¸ Video rendered, finalizing{timestamp_str}")
                if generated_code := status_data.get("generatedCode"):
                    print(f"ğŸ“ Generated code available")
            elif status == "SUCCEEDED":
                video_url = status_data.get("videoUrl")
                print(f"âœ… Video generation complete!{timestamp_str}")
                print(f"ğŸ¬ Video URL: {video_url}")
                
                # Print processing time if available
                if processing_time := status_data.get("processingTime"):
                    print(f"â±ï¸  Processing time: {processing_time} seconds")
                
                # Use macOS "say" command if available
                if platform.system() == "Darwin":  # Darwin is the system name for macOS
                    try:
                        subprocess.run(["say", "Your video is ready"], check=False)
                    except Exception as e:
                        # Silently fail if the say command doesn't work
                        pass
                    
                break
                
            elif status == "FAILED":
                error = status_data.get("error") or "Unknown error"
                print(f"âŒ Generation failed: {error}{timestamp_str}")
                break
            
            last_status = status
            
        # Wait before next check
        time.sleep(3)
        
except KeyboardInterrupt:
    print("\nğŸ›‘ Process interrupted by user")
    sys.exit(0)
    
except Exception as e:
    print(f"\nâŒ Error: {str(e)}")
    sys.exit(1) 


================================================
FILE: animo/examples/sample.py
================================================
from animo import Animo
import os
from pprint import pprint

# Get API key from environment variable (recommended)
# You can also hardcode it, but it's not recommended for security reasons
api_key = os.getenv("ANIMO_API_KEY")

# Initialize the Animo client
client = Animo(api_key=api_key)

# Define the Manim scene code
# This will create a scene that writes "Arte de programar" with an animation
code = """
from manim import *

class GenScene(Scene):
    def construct(self):
        text = Text("Arte de programar", font_size=72, color=WHITE)
        self.play(Write(text))
"""

print("ğŸ¬ Creating video...")

try:
    # Create the video
    # You can customize:
    # - file_class: The name of your scene class
    # - aspect_ratio: "16:9" (default), "1:1", or "9:16"
    # - stream: Set to True if you want to receive progress updates
    response = client.videos.create(
        code=code,
        file_class="GenScene",
        aspect_ratio="16:9",
        stream=False
    )
    
    # Print the full response for debugging
    print("\nğŸ“‹ API Response:")
    pprint(response)
    
    # Get the video URL
    video_url = response.get("video_url")
    
    if video_url:
        print(f"\nâœ… Success! Your video is ready at:\n{video_url}")
    else:
        print("\nâŒ Error: No video URL in the response")

except Exception as e:
    print(f"\nâŒ Error: {str(e)}") 


================================================
FILE: animo/src/animo/__init__.py
================================================
"""
Animo - Create animations from text using Manim
"""

from .client import Animo

__version__ = "0.1.4"
__all__ = ["Animo"] 


================================================
FILE: animo/src/animo/client.py
================================================
from .resources.videos import Videos

class Animo:
    """
    Animo client for interacting with the Animo API.
    """
    
    def __init__(self, api_key: str, base_url: str = "https://api.animo.video"):
        """
        Initialize the Animo client.

        Args:
            api_key (str): Your API key for authentication
            base_url (str, optional): The base URL for the API. 
                Defaults to "https://api.animo.video"
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        
        # Initialize resources
        self.videos = Videos(self) 


================================================
FILE: animo/src/animo/resources/__init__.py
================================================
"""
Animo API resources
""" 


================================================
FILE: animo/src/animo/resources/videos.py
================================================
import requests
from typing import Dict, Any

class Videos:
    """
    Handle video-related operations with the Animo API.
    """
    
    def __init__(self, client):
        self.client = client

    def create(
        self, 
        code: str, 
        file_class: str = "GenScene",
        aspect_ratio: str = "16:9",
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        Create a video by rendering Manim code.

        Args:
            code (str): The Manim Python code to render
            file_class (str, optional): The Manim scene class name. Defaults to "GenScene"
            aspect_ratio (str, optional): Video aspect ratio ("16:9", "1:1", "9:16"). Defaults to "16:9"
            stream (bool, optional): Whether to stream the rendering progress. Defaults to False

        Returns:
            Dict[str, Any]: The API response containing the video URL
        """
        response = requests.post(
            f"{self.client.base_url}/v1/video/rendering",
            headers={
                "Authorization": f"Bearer {self.client.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "code": code,
                "file_class": file_class,
                "aspect_ratio": aspect_ratio,
                "stream": stream
            }
        )
        response.raise_for_status()
        return response.json()

    def generate(
        self,
        prompt: str,
        engine: str = "anthropic",
        model: str = "claude-3-7-sonnet-20250219"
    ) -> Dict[str, Any]:
        """
        Generate a video from a text prompt using AI.

        Args:
            prompt (str): The text prompt describing the animation to create
            engine (str, optional): The AI engine to use. Defaults to "anthropic"
            model (str, optional): The AI model to use. Defaults to "claude-3-7-sonnet-20250219"

        Returns:
            Dict[str, Any]: The API response containing the request ID and status
        """
        response = requests.post(
            f"{self.client.base_url}/v1/video/generation",
            headers={
                "Authorization": f"Bearer {self.client.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "prompt": prompt,
                "engine": engine,
                "model": model
            }
        )
        response.raise_for_status()
        return response.json()

    def retrieve(self, request_id: str) -> Dict[str, Any]:
        """
        Retrieve the status and result of a video generation request.

        Args:
            request_id (str): The ID of the generation request to check

        Returns:
            Dict[str, Any]: The API response containing the status and video URL if completed
        """
        response = requests.get(
            f"{self.client.base_url}/v1/video/generation/status/{request_id}",
            headers={
                "Authorization": f"Bearer {self.client.api_key}",
                "Content-Type": "application/json"
            }
        )
        response.raise_for_status()
        return response.json()

    def export(self, scenes: list, title_slug: str) -> Dict[str, Any]:
        """
        Export multiple scenes into a single video.

        Args:
            scenes (list): List of scene objects containing videoUrl
            title_slug (str): Slug for the exported video title

        Returns:
            Dict[str, Any]: The API response containing the exported video URL
        """
        response = requests.post(
            f"{self.client.base_url}/v1/video/exporting",
            headers={
                "Authorization": f"Bearer {self.client.api_key}",
                "Content-Type": "application/json"
            },
            json={
                "scenes": scenes,
                "titleSlug": title_slug
            }
        )
        response.raise_for_status()
        return response.json() 


================================================
FILE: api/README.md
================================================
# ğŸ”¬ Generative Manim API (Animation Processing Interface)

The Animation Processing Interface or API is a REST API that can be used to generate Manim scripts using LLMs and render videos from Python code. This is the API used on [Generative Manim Demo](https://generative-manim.vercel.app/) and [Animo](https://animo.video) under the hood.

## ğŸš€ Concept

We are creating the software that enables you to transform ideas, concepts, and more into animated videos.

Generative Manim API (Animation Processing Interface) empowers you to generate Manim scripts using LLMs and render videos from Python code. This allows seamless integration into your website, app, or any kind of project that requires animations. Happy coding!

## ğŸ¤– Animo

[Animo](https://animo.video) allows you to use this API as your own Animation Processing Interface. That way, you can have more control and flexibility over the animations you generate. To connect to your own server from Animo, set up this server, and enable **Use Local Server** in the **Settings** tab of your Animo project.

![Preview on Use Local Server](./../.github/use_local_server.png)

## ğŸ“¦ Installation

### Prerequisites

Altough Docker is not required, it is recommended to make sure you have the necessary tools to render videos or to generate code.

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- [Manim](https://docs.manim.community/en/stable/installation.html)

And, depending on the model you want to use, you will need to have an API key for the model provider.

- [OpenAI](https://openai.com/api/)
- [Anthropic](https://www.anthropic.com/api)

If you are on MacOS, make sure you have `pkg-config` and `cairo` installed. You can install them using Homebrew:

```bash
brew install pkg-config
brew install cairo
```

### Steps

1. **Clone the repository:**

```bash
git clone https://github.com/marcelo-earth/generative-manim.git
```

2. **Install the requirements on the `/api` directory:**

```bash
cd api
pip install -r requirements.txt
```

Now you have the option of running the API locally or using Docker.

### Running the API locally

3. Run the `run.py` script to start the API server.

```bash
cd generative-manim
python run.py
```

### Running the API using Docker

**Build the Docker image:**

3. Run the following command from the root directory of the repository.

```bash
cd generative-manim
docker build -t generative-manim-api .
```

4. **Run the Docker container:**

```bash
docker run -p 8080:8080 generative-manim-api
```

5. **Wohoo!** ğŸ‰ Congratulations! You have the API running.

Now that you have the API running, you can use it to generate Manim scripts and render videos. Or you can interact with it using the [Animo](https://animo.video) platform. Remember to enable **Use Local Server** in the **Settings** tab of your Animo project. And when required, paste the HTTP URL to the API.

## ğŸ“ Usage

### ğŸ’» How to generate Manim Code?

Let's say you want to create a simple animation that shows a square transforming into a circle. You can use the `/v1/code/generation` endpoint with a POST request. Like this:

```bash
curl -X POST http://127.0.0.1:8080/v1/code/generation \
-H "Content-Type: application/json" \
-d '{
  "prompt": "Create a Manim animation that shows a square transforming into a circle"
}'
```

### ğŸ’¬ How to generate Manim Code via chat?

Let's say you want to generate a simple animation of a 3D rotating cube. You can use the `/v1/chat/generation` endpoint with a POST request. Like this:

```bash
curl -N -X POST http://127.0.0.1:8080/v1/chat/generation \
-H "Content-Type: application/json" \
-d '{
  "messages": [
    {
      "role": "user",
      "content": "Create a simple Manim animation that shows a circle transforming into a square."
    }
  ],
  "engine": "anthropic"
}'
```

When executed, the API will return the stream of the latest assistant message.

### ğŸ¥ How to render a Manim video?

Let's say you want to render a circle scaling animation. You can use the `/v1/video/rendering` endpoint with a POST request. Like this:

```bash
curl -X POST http://127.0.0.1:8080/v1/video/rendering \
-H "Content-Type: application/json" \
-d '{
  "code": "from manim import *\n\nclass DrawCircle(Scene):\n    def construct(self):\n        # Create a circle object\n        circle = Circle()\n\n        # Use the animate API to smoothly scale the circle down to a radius of 0.1\n        self.play(Create(circle))\n",
  "file_name": "scene_56861",
  "file_class": "DrawCircle",
  "stream": true
}'
```



================================================
FILE: api/__init__.py
================================================
from flask import Flask, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv
from .routes.video_rendering import video_rendering_bp
from .routes.code_generation import code_generation_bp
from .routes.chat_generation import chat_generation_bp

def create_app():
    app = Flask(__name__, static_folder="public", static_url_path="/public")

    load_dotenv()

    app.register_blueprint(video_rendering_bp)
    app.register_blueprint(code_generation_bp)
    app.register_blueprint(chat_generation_bp)

    CORS(app)
    
    @app.route("/")
    def hello_world():
        return "Generative Manim Processor"
    
    @app.route("/openapi.yaml")
    def openapi():
        return send_from_directory(app.static_folder, "openapi.yaml")

    return app



================================================
FILE: api/requirements.txt
================================================
flask==2.2.2
gunicorn==20.1.0
werkzeug==2.3.7
ffmpeg==1.4
openai>=1.24.0,<2.0.0
manim==0.18.0
google-cloud-storage==2.10.0
python-dotenv==1.0.0
bs4==0.0.1
langchain==0.1.16
langchain_community==0.0.34
langchain-openai==0.1.6
langchain-anthropic==0.1.11
langgraph==0.0.38
anthropic==0.37.1
qianfan==0.3.11
google-cloud-logging==3.10.0
flask-cors==3.0.10
azure-storage-blob==12.18.2
azure-monitor-opentelemetry==1.4.2
manim-physics==0.4.0


================================================
FILE: api/run.py
================================================
import os
from api import create_app

app = create_app()

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(debug=False, host="0.0.0.0", port=port)



================================================
FILE: api/.env.example
================================================
AZURE_STORAGE_CONNECTION_STRING=your_azure_storage_connection_string
AZURE_STORAGE_CONTAINER_NAME=your_azure_storage_container_name
PORT=8080
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
USE_LOCAL_STORAGE=true
BASE_URL="http://127.0.0.1:8080"



================================================
FILE: api/prompts/manimDocs.py
================================================
manimDocs = """
## Manim Index Documentation

This reference list details modules, functions, and variables included in Manim, describing what they are and what they do.

- [Animations](reference_index/animations.html)
  - [animation](reference/manim.animation.animation.html)
    - [Animation](reference/manim.animation.animation.Animation.html)
    - [Wait](reference/manim.animation.animation.Wait.html)
    - [override_animation()](reference/manim.animation.animation.html#manim.animation.animation.override_animation)
    - [prepare_animation()](reference/manim.animation.animation.html#manim.animation.animation.prepare_animation)
  - [changing](reference/manim.animation.changing.html)
    - [AnimatedBoundary](reference/manim.animation.changing.AnimatedBoundary.html)
    - [TracedPath](reference/manim.animation.changing.TracedPath.html)
  - [composition](reference/manim.animation.composition.html)
    - [AnimationGroup](reference/manim.animation.composition.AnimationGroup.html)
    - [LaggedStart](reference/manim.animation.composition.LaggedStart.html)
    - [LaggedStartMap](reference/manim.animation.composition.LaggedStartMap.html)
    - [Succession](reference/manim.animation.composition.Succession.html)
  - [creation](reference/manim.animation.creation.html)
    - [AddTextLetterByLetter](reference/manim.animation.creation.AddTextLetterByLetter.html)
    - [AddTextWordByWord](reference/manim.animation.creation.AddTextWordByWord.html)
    - [Create](reference/manim.animation.creation.Create.html)
    - [DrawBorderThenFill](reference/manim.animation.creation.DrawBorderThenFill.html)
    - [RemoveTextLetterByLetter](reference/manim.animation.creation.RemoveTextLetterByLetter.html)
    - [ShowIncreasingSubsets](reference/manim.animation.creation.ShowIncreasingSubsets.html)
    - [ShowPartial](reference/manim.animation.creation.ShowPartial.html)
    - [ShowSubmobjectsOneByOne](reference/manim.animation.creation.ShowSubmobjectsOneByOne.html)
    - [SpiralIn](reference/manim.animation.creation.SpiralIn.html)
    - [Uncreate](reference/manim.animation.creation.Uncreate.html)
    - [Unwrite](reference/manim.animation.creation.Unwrite.html)
    - [Write](reference/manim.animation.creation.Write.html)
  - [fading](reference/manim.animation.fading.html)
    - [FadeIn](reference/manim.animation.fading.FadeIn.html)
    - [FadeOut](reference/manim.animation.fading.FadeOut.html)
  - [growing](reference/manim.animation.growing.html)
    - [GrowArrow](reference/manim.animation.growing.GrowArrow.html)
    - [GrowFromCenter](reference/manim.animation.growing.GrowFromCenter.html)
    - [GrowFromEdge](reference/manim.animation.growing.GrowFromEdge.html)
    - [GrowFromPoint](reference/manim.animation.growing.GrowFromPoint.html)
    - [SpinInFromNothing](reference/manim.animation.growing.SpinInFromNothing.html)
  - [indication](reference/manim.animation.indication.html)
    - [ApplyWave](reference/manim.animation.indication.ApplyWave.html)
    - [Circumscribe](reference/manim.animation.indication.Circumscribe.html)
    - [Flash](reference/manim.animation.indication.Flash.html)
    - [FocusOn](reference/manim.animation.indication.FocusOn.html)
    - [Indicate](reference/manim.animation.indication.Indicate.html)
    - [ShowPassingFlash](reference/manim.animation.indication.ShowPassingFlash.html)
    - [ShowPassingFlashWithThinningStrokeWidth](reference/manim.animation.indication.ShowPassingFlashWithThinningStrokeWidth.html)
    - [Wiggle](reference/manim.animation.indication.Wiggle.html)
  - [movement](reference/manim.animation.movement.html)
    - [ComplexHomotopy](reference/manim.animation.movement.ComplexHomotopy.html)
    - [Homotopy](reference/manim.animation.movement.Homotopy.html)
    - [MoveAlongPath](reference/manim.animation.movement.MoveAlongPath.html)
    - [PhaseFlow](reference/manim.animation.movement.PhaseFlow.html)
    - [SmoothedVectorizedHomotopy](reference/manim.animation.movement.SmoothedVectorizedHomotopy.html)
  - [numbers](reference/manim.animation.numbers.html)
    - [ChangeDecimalToValue](reference/manim.animation.numbers.ChangeDecimalToValue.html)
    - [ChangingDecimal](reference/manim.animation.numbers.ChangingDecimal.html)
  - [rotation](reference/manim.animation.rotation.html)
    - [Rotate](reference/manim.animation.rotation.Rotate.html)
    - [Rotating](reference/manim.animation.rotation.Rotating.html)
  - [specialized](reference/manim.animation.specialized.html)
    - [Broadcast](reference/manim.animation.specialized.Broadcast.html)
  - [speedmodifier](reference/manim.animation.speedmodifier.html)
    - [ChangeSpeed](reference/manim.animation.speedmodifier.ChangeSpeed.html)
  - [transform](reference/manim.animation.transform.html)
    - [ApplyComplexFunction](reference/manim.animation.transform.ApplyComplexFunction.html)
    - [ApplyFunction](reference/manim.animation.transform.ApplyFunction.html)
    - [ApplyMatrix](reference/manim.animation.transform.ApplyMatrix.html)
    - [ApplyMethod](reference/manim.animation.transform.ApplyMethod.html)
    - [ApplyPointwiseFunction](reference/manim.animation.transform.ApplyPointwiseFunction.html)
    - [ApplyPointwiseFunctionToCenter](reference/manim.animation.transform.ApplyPointwiseFunctionToCenter.html)
    - [ClockwiseTransform](reference/manim.animation.transform.ClockwiseTransform.html)
    - [CounterclockwiseTransform](reference/manim.animation.transform.CounterclockwiseTransform.html)
    - [CyclicReplace](reference/manim.animation.transform.CyclicReplace.html)
    - [FadeToColor](reference/manim.animation.transform.FadeToColor.html)
    - [FadeTransform](reference/manim.animation.transform.FadeTransform.html)
    - [FadeTransformPieces](reference/manim.animation.transform.FadeTransformPieces.html)
    - [MoveToTarget](reference/manim.animation.transform.MoveToTarget.html)
    - [ReplacementTransform](reference/manim.animation.transform.ReplacementTransform.html)
    - [Restore](reference/manim.animation.transform.Restore.html)
    - [ScaleInPlace](reference/manim.animation.transform.ScaleInPlace.html)
    - [ShrinkToCenter](reference/manim.animation.transform.ShrinkToCenter.html)
    - [Swap](reference/manim.animation.transform.Swap.html)
    - [Transform](reference/manim.animation.transform.Transform.html)
    - [TransformAnimations](reference/manim.animation.transform.TransformAnimations.html)
    - [TransformFromCopy](reference/manim.animation.transform.TransformFromCopy.html)
  - [transform_matching_parts](reference/manim.animation.transform_matching_parts.html)
    - [TransformMatchingAbstractBase](reference/manim.animation.transform_matching_parts.TransformMatchingAbstractBase.html)
    - [TransformMatchingShapes](reference/manim.animation.transform_matching_parts.TransformMatchingShapes.html)
    - [TransformMatchingTex](reference/manim.animation.transform_matching_parts.TransformMatchingTex.html)
  - [updaters](reference/manim.animation.updaters.html)
    - [Modules](reference/manim.animation.updaters.html#modules)
- [Cameras](reference_index/cameras.html)
  - [camera](reference/manim.camera.camera.html)
    - [BackgroundColoredVMobjectDisplayer](reference/manim.camera.camera.BackgroundColoredVMobjectDisplayer.html)
    - [Camera](reference/manim.camera.camera.Camera.html)
  - [mapping_camera](reference/manim.camera.mapping_camera.html)
    - [MappingCamera](reference/manim.camera.mapping_camera.MappingCamera.html)
    - [OldMultiCamera](reference/manim.camera.mapping_camera.OldMultiCamera.html)
    - [SplitScreenCamera](reference/manim.camera.mapping_camera.SplitScreenCamera.html)
  - [moving_camera](reference/manim.camera.moving_camera.html)
    - [MovingCamera](reference/manim.camera.moving_camera.MovingCamera.html)
  - [multi_camera](reference/manim.camera.multi_camera.html)
    - [MultiCamera](reference/manim.camera.multi_camera.MultiCamera.html)
  - [three_d_camera](reference/manim.camera.three_d_camera.html)
    - [ThreeDCamera](reference/manim.camera.three_d_camera.ThreeDCamera.html)
- [Configuration](reference_index/configuration.html)
  - [Module Index](reference_index/configuration.html#module-index)
    - [\_config](reference/manim._config.html)
    - [utils](reference/manim._config.utils.html)
    - [logger_utils](reference/manim._config.logger_utils.html)
- [Mobjects](reference_index/mobjects.html)
  - [frame](reference/manim.mobject.frame.html)
    - [FullScreenRectangle](reference/manim.mobject.frame.FullScreenRectangle.html)
    - [ScreenRectangle](reference/manim.mobject.frame.ScreenRectangle.html)
  - [geometry](reference/manim.mobject.geometry.html)
    - [Modules](reference/manim.mobject.geometry.html#modules)
  - [graph](reference/manim.mobject.graph.html)
    - [NxGraph](reference/manim.mobject.graph.html#manim.mobject.graph.NxGraph)
    - [DiGraph](reference/manim.mobject.graph.DiGraph.html)
    - [GenericGraph](reference/manim.mobject.graph.GenericGraph.html)
    - [Graph](reference/manim.mobject.graph.Graph.html)
    - [LayoutFunction](reference/manim.mobject.graph.LayoutFunction.html)
  - [graphing](reference/manim.mobject.graphing.html)
    - [Modules](reference/manim.mobject.graphing.html#modules)
  - [logo](reference/manim.mobject.logo.html)
    - [ManimBanner](reference/manim.mobject.logo.ManimBanner.html)
  - [matrix](reference/manim.mobject.matrix.html)
    - [DecimalMatrix](reference/manim.mobject.matrix.DecimalMatrix.html)
    - [IntegerMatrix](reference/manim.mobject.matrix.IntegerMatrix.html)
    - [Matrix](reference/manim.mobject.matrix.Matrix.html)
    - [MobjectMatrix](reference/manim.mobject.matrix.MobjectMatrix.html)
    - [get_det_text()](reference/manim.mobject.matrix.html#manim.mobject.matrix.get_det_text)
    - [matrix_to_mobject()](reference/manim.mobject.matrix.html#manim.mobject.matrix.matrix_to_mobject)
    - [matrix_to_tex_string()](reference/manim.mobject.matrix.html#manim.mobject.matrix.matrix_to_tex_string)
  - [mobject](reference/manim.mobject.mobject.html)
    - [TimeBasedUpdater](reference/manim.mobject.mobject.html#manim.mobject.mobject.TimeBasedUpdater)
    - [NonTimeBasedUpdater](reference/manim.mobject.mobject.html#manim.mobject.mobject.NonTimeBasedUpdater)
    - [Updater](reference/manim.mobject.mobject.html#manim.mobject.mobject.Updater)
    - [Group](reference/manim.mobject.mobject.Group.html)
    - [Mobject](reference/manim.mobject.mobject.Mobject.html)
    - [override_animate()](reference/manim.mobject.mobject.html#manim.mobject.mobject.override_animate)
  - [svg](reference/manim.mobject.svg.html)
    - [Modules](reference/manim.mobject.svg.html#modules)
  - [table](reference/manim.mobject.table.html)
    - [DecimalTable](reference/manim.mobject.table.DecimalTable.html)
    - [IntegerTable](reference/manim.mobject.table.IntegerTable.html)
    - [MathTable](reference/manim.mobject.table.MathTable.html)
    - [MobjectTable](reference/manim.mobject.table.MobjectTable.html)
    - [Table](reference/manim.mobject.table.Table.html)
  - [text](reference/manim.mobject.text.html)
    - [Modules](reference/manim.mobject.text.html#modules)
  - [three_d](reference/manim.mobject.three_d.html)
    - [Modules](reference/manim.mobject.three_d.html#modules)
  - [types](reference/manim.mobject.types.html)
    - [Modules](reference/manim.mobject.types.html#modules)
  - [utils](reference/manim.mobject.utils.html)
    - [get_mobject_class()](reference/manim.mobject.utils.html#manim.mobject.utils.get_mobject_class)
    - [get_point_mobject_class()](reference/manim.mobject.utils.html#manim.mobject.utils.get_point_mobject_class)
    - [get_vectorized_mobject_class()](reference/manim.mobject.utils.html#manim.mobject.utils.get_vectorized_mobject_class)
  - [value_tracker](reference/manim.mobject.value_tracker.html)
    - [ComplexValueTracker](reference/manim.mobject.value_tracker.ComplexValueTracker.html)
    - [ValueTracker](reference/manim.mobject.value_tracker.ValueTracker.html)
  - [vector_field](reference/manim.mobject.vector_field.html)
    - [ArrowVectorField](reference/manim.mobject.vector_field.ArrowVectorField.html)
    - [StreamLines](reference/manim.mobject.vector_field.StreamLines.html)
    - [VectorField](reference/manim.mobject.vector_field.VectorField.html)
- [Scenes](reference_index/scenes.html)
  - [moving_camera_scene](reference/manim.scene.moving_camera_scene.html)
    - [MovingCameraScene](reference/manim.scene.moving_camera_scene.MovingCameraScene.html)
  - [section](reference/manim.scene.section.html)
    - [DefaultSectionType](reference/manim.scene.section.DefaultSectionType.html)
    - [Section](reference/manim.scene.section.Section.html)
  - [scene](reference/manim.scene.scene.html)
    - [RerunSceneHandler](reference/manim.scene.scene.RerunSceneHandler.html)
    - [Scene](reference/manim.scene.scene.Scene.html)
  - [scene_file_writer](reference/manim.scene.scene_file_writer.html)
    - [SceneFileWriter](reference/manim.scene.scene_file_writer.SceneFileWriter.html)
  - [three_d_scene](reference/manim.scene.three_d_scene.html)
    - [SpecialThreeDScene](reference/manim.scene.three_d_scene.SpecialThreeDScene.html)
    - [ThreeDScene](reference/manim.scene.three_d_scene.ThreeDScene.html)
  - [vector_space_scene](reference/manim.scene.vector_space_scene.html)
    - [LinearTransformationScene](reference/manim.scene.vector_space_scene.LinearTransformationScene.html)
    - [VectorScene](reference/manim.scene.vector_space_scene.VectorScene.html)
  - [zoomed_scene](reference/manim.scene.zoomed_scene.html)
    - [ZoomedScene](reference/manim.scene.zoomed_scene.ZoomedScene.html)
- [Utilities and other modules](reference_index/utilities_misc.html)
  - [Module Index](reference_index/utilities_misc.html#module-index)
    - [bezier](reference/manim.utils.bezier.html)
    - [color](reference/manim.utils.color.html)
    - [commands](reference/manim.utils.commands.html)
    - [config_ops](reference/manim.utils.config_ops.html)
    - [constants](reference/manim.constants.html)
    - [debug](reference/manim.utils.debug.html)
    - [deprecation](reference/manim.utils.deprecation.html)
    - [docbuild](reference/manim.utils.docbuild.html)
    - [hashing](reference/manim.utils.hashing.html)
    - [images](reference/manim.utils.images.html)
    - [ipython_magic](reference/manim.utils.ipython_magic.html)
    - [iterables](reference/manim.utils.iterables.html)
    - [paths](reference/manim.utils.paths.html)
    - [rate_functions](reference/manim.utils.rate_functions.html)
    - [simple_functions](reference/manim.utils.simple_functions.html)
    - [sounds](reference/manim.utils.sounds.html)
    - [space_ops](reference/manim.utils.space_ops.html)
    - [testing](reference/manim.utils.testing.html)
    - [tex](reference/manim.utils.tex.html)
    - [tex_file_writing](reference/manim.utils.tex_file_writing.html)
    - [tex_templates](reference/manim.utils.tex_templates.html)
    - [typing](reference/manim.typing.html)
"""



================================================
FILE: api/public/openapi.yaml
================================================
openapi: 3.0.1
info:
  title: Generative Manim API
  description: API for generating Manim code, converting it to video, and handling chat-based code generation.
  version: 1.0.0
  license:
    name: Apache License, Version 2.0
    url: http://www.apache.org/licenses/LICENSE-2.0.html

servers:
  - url: http://localhost:8080

paths:
  /generate-code:
    post:
      summary: Generate Manim Code
      description: Generates Manim code based on a provided prompt using either OpenAI or Anthropic models.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                prompt:
                  type: string
                  description: The prompt to generate Manim code.
                  example: Create a blue circle and animate its creation.
                model:
                  type: string
                  description: The model to use for code generation (e.g., 'gpt-4o' for OpenAI or 'claude-3-haiku-20240307' for Anthropic).
                  example: gpt-4o
      responses:
        '200':
          description: Successful response with generated code
          content:
            application/json:
              schema:
                type: object
                properties:
                  code:
                    type: string
                    description: The generated Manim code.
        '500':
          description: Internal Server Error
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string

  /chat:
    post:
      summary: Chat for Code Generation
      description: Handles chat requests for both Anthropic and OpenAI models to assist with Manim code generation.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                messages:
                  type: array
                  items:
                    type: object
                    properties:
                      role:
                        type: string
                      content:
                        oneOf:
                          - type: string
                          - type: array
                            items:
                              type: object
                engine:
                  type: string
                  enum: [openai, anthropic]
                  default: openai
      responses:
        '200':
          description: Successful response with streaming content
          content:
            application/json:
              schema:
                type: object
                properties:
                  stream:
                    type: boolean
                  response:
                    type: object
        '500':
          description: Internal Server Error
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string

  /v1/video/rendering:
    post:
      summary: Convert Code to Video
      description: Converts provided Manim code to a video and uploads it to storage.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                code:
                  type: string
                  description: The Manim code to convert to video.
                file_name:
                  type: string
                  description: The name of the file to save the code.
                file_class:
                  type: string
                  description: The class name to render.
                user_id:
                  type: string
                  description: The user ID for the video.
                  default: "anonymous"
                project_name:
                  type: string
                  description: The project name for the video.
                iteration:
                  type: integer
                  description: The iteration number for naming the video.
                aspect_ratio:
                  type: string
                  description: The aspect ratio of the video (16:9, 1:1, or 9:16).
                  default: "16:9"
                stream:
                  type: boolean
                  description: Whether to stream the percentage of animation shown.
                  default: false
      responses:
        '200':
          description: Successful response with video URL
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
                    description: Success message.
                  video_url:
                    type: string
                    description: URL of the generated video.
        '207':
          description: Streaming response with animation progress
          content:
            text/event-stream:
              schema:
                type: string
                description: JSON-encoded progress updates or error messages.
        '400':
          description: Bad Request
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                    description: Error message.
        '500':
          description: Internal Server Error
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
                    description: Error message.

  /v1/chat/generation:
    post:
      summary: Generate Chat-based Manim Code
      description: Generates Manim code based on a chat conversation using either OpenAI or Anthropic models.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                messages:
                  type: array
                  items:
                    type: object
                    properties:
                      role:
                        type: string
                      content:
                        oneOf:
                          - type: string
                          - type: array
                            items:
                              type: object
                globalPrompt:
                  type: string
                userId:
                  type: string
                scenes:
                  type: array
                  items:
                    type: object
                projectTitle:
                  type: string
                engine:
                  type: string
                  enum: [openai, anthropic]
                  default: openai
                selectedScenes:
                  type: array
                  items:
                    type: string
      responses:
        '200':
          description: Successful streaming response with generated content
          content:
            text/event-stream:
              schema:
                type: string
                description: Streamed content from the AI model.
        '500':
          description: Internal Server Error
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string

components:
  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key

security:
  - ApiKeyAuth: []



================================================
FILE: api/routes/__init__.py
================================================
[Empty file]


================================================
FILE: api/routes/chat_generation.py
================================================
from flask import Blueprint, jsonify, request, Response, stream_with_context
import anthropic
import openai
import os
import json
import subprocess
import shutil
import string
import random
import re
import base64
from api.prompts.manimDocs import manimDocs
from azure.storage.blob import BlobServiceClient
from PIL import Image
import io
import time
from openai import APIError
import uuid

chat_generation_bp = Blueprint("chat_generation", __name__)


animo_functions = {
    "openai": [
        {
            "name": "get_preview",
            "description": "Get a preview of the video animation before giving it. Use this function always, before giving the final code to the user. And use it to generate frames of the video, so you can see it and improve it over time. Also, before using this function, tell the user you will be generating a preview based on the code they see. Always use spaces to maintain the indentation. Indentation is important, otherwise the code will not work.",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The code to get the preview of. Take account the spaces to maintain the indentation.",
                    },
                    "class_name": {
                        "type": "string",
                        "description": "The name of the class to get the preview of. The name of the class should be the same as the name of the class in the code.",
                    }
                },
                "required": ["code", "class_name"],
            },
            "output": {"type": "string", "description": "Images URLs of the animation that will be inserted in the conversation"},
        }
    ],
    "anthropic": [
        {
            "name": "get_preview",
            "description": "Get a preview of the video animation before giving it. Use this function always, before giving the final code to the user. And use it to generate frames of the video, so you can see it and improve it over time. Also, before using this function, tell the user you will be generating a preview based on the code they see. Always use spaces to maintain the indentation. Indentation is important, otherwise the code will not work.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The code to get the preview of. Take account the spaces to maintain the indentation."
                    },
                    "class_name": {
                        "type": "string",
                        "description": "The name of the class to get the preview of. The name of the class should be the same as the name of the class in the code."
                    }
                },
                "required": ["code", "class_name"]
            }
        }
    ]
}

def count_images_in_conversation(messages):
    """
    Count the total number of images in the conversation.
    Returns a tuple of (total_count, list of image messages indices)
    """
    total_images = 0
    image_message_indices = []
    
    for i, message in enumerate(messages):
        if message.get("role") == "user" and isinstance(message.get("content"), list):
            image_count = sum(1 for content in message["content"] if isinstance(content, dict) and content.get("type") == "image_url")
            if image_count > 0:
                total_images += image_count
                image_message_indices.append(i)
    
    return total_images, image_message_indices

def manage_conversation_images(messages, new_images_count, engine):
    """
    Manage the conversation to ensure we don't exceed image limits.
    For OpenAI, we maintain only the last 50 images.
    Returns the maximum number of new images we can add.
    """
    if engine != "openai":
        return len(new_images_count)  # No limit for other engines
        
    MAX_IMAGES = 50
    current_total, image_indices = count_images_in_conversation(messages)
    
    # If we already have too many images, remove old image messages
    while current_total > 0 and current_total + new_images_count > MAX_IMAGES and image_indices:
        oldest_image_idx = image_indices[0]
        removed_message = messages.pop(oldest_image_idx)
        # Recount images in the removed message
        removed_images = sum(1 for content in removed_message["content"] 
                           if isinstance(content, dict) and content.get("type") == "image_url")
        current_total -= removed_images
        # Update indices after removal
        image_indices = [idx - 1 for idx in image_indices[1:]]  # Adjust remaining indices
    
    # Return how many new images we can add
    return min(MAX_IMAGES - current_total, new_images_count)

@chat_generation_bp.route("/v1/chat/generation", methods=["POST"])
def generate_code_chat():
    """
    This endpoint generates code for animations using OpenAI or Anthropic.
    It supports both OpenAI and Anthropic models and returns a stream of content.
    
    When calling this endpoint, enable `is_for_platform` to interact with the platform 'Animo'.
    """
    print("Received request for /v1/chat/generation")

    data = request.json
    print(f"Request data: {json.dumps(data, indent=2)}")

    messages = data.get("messages", [])
    prompt = data.get("prompt")
    global_prompt = data.get("globalPrompt", "")
    user_id = data.get("userId") or f"user-{uuid.uuid4()}"
    scenes = data.get("scenes", [])
    project_title = data.get("projectTitle", "")
    engine = data.get("engine", "openai")
    model = data.get("model", None)  # Optional model parameter
    selected_scenes = data.get("selectedScenes", [])
    is_for_platform = data.get("isForPlatform", False)

    # Define default models for each engine
    ENGINE_DEFAULTS = {
        "openai": "gpt-4o",
        "anthropic": "claude-35-sonnet",
        "deepseek": "r1"
    }

    # Validate and set the model based on engine
    if engine not in ENGINE_DEFAULTS:
        return jsonify({"error": f"Invalid engine. Must be one of: {', '.join(ENGINE_DEFAULTS.keys())}"}), 400

    # If no model specified, use the default for the engine
    if not model:
        model = ENGINE_DEFAULTS[engine]

    # Validate model based on engine
    VALID_MODELS = {
        "openai": ["gpt-4o", "o1-mini"],
        "anthropic": ["claude-35-sonnet"],
        "deepseek": ["r1"]
    }

    if model not in VALID_MODELS[engine]:
        return jsonify({
            "error": f"Invalid model '{model}' for engine '{engine}'. Valid models are: {', '.join(VALID_MODELS[engine])}"
        }), 400

    if not messages and prompt:
        messages = [{"role": "user", "content": prompt}]
    
    print("messages")
    print(messages)

    general_system_prompt = """You are an assistant that creates animations with Manim. Manim is a mathematical animation engine that is used to create videos programmatically. You are running on Animo (www.animo.video), a tool to create videos with Manim.

# What the user can do?

The user can create a new project, add scenes, and generate the video. You can help the user to generate the video by creating the code for the scenes. The user can add custom rules for you, can select a different aspect ratio, and can change the model (the models are: OpenAI GPT-4o, and Anthropic Claude 3.5 Sonnet).

# Project

A project can be composed of multiple scenes. This current project (where the user is working on right now) is called '{project_title}', and the following scenes are part of this project. The purpose of showing the list of scenes is to keep the context of the whole video project.

## List of scenes:
{scenes_prompt}

# Behavior Context

The user will ask you to generatte an animation, you should iterate while using the `get_preview` function. This function will generate a preview of the animation, and will be inserted in the conversation so you can see the frames of it, and enhance it across the time. You can make this iteration up to 4 times without user confirmation. Just use the `get_preview` until you are sure that the animation is ready.

FAQ
**Should the assistant generate the code first and then use the `get_preview` function?**
No, unless the user asks for it. Always use the `get_preview` function to generate the code. The user will see the code anyway, so there is no need to duplicate the work. Use the `get_preview` as your way to quickly draft the code and then iterate on it.

**Can the user see the code generated?**
Yes, even if you use the `get_preview` function, the code will be generated and visible for the user.

**Can the assistant propose a more efficient way to generate the animation?**
Yes, the assistant can propose a more efficient way to generate the animation. For example, the assistant can propose a different aspect ratio, a different model, or a different scene. If the change is too big, you should ask the user for confirmation. Act with initiative.

**Should the assistant pause to change the code?**
Yes, always stay in the loop of generating the preview and improving it from what you see.
Incorrect: Please hold on while I make these adjustments.
Correct: Now I will do the adjustments *and does the adjustments*.

**Should the assistant tell the user about the get_preview function?**
Yes, here are some examples:

1. Let me generate a preview of the animation to see how it looks like for you. I'll see it.
2. I have an idea on how to improve the animation, let me visualize it for a second.
3. OK. Now I know how to improve the animation. Please give me a moment to preview it.

# Code Context

The following is an example of the code:
\`\`\`
from manim import *
from math import *

class GenScene(Scene):
  def construct(self):
      # Create a circle of color BLUE
      c = Circle(color=BLUE)
      # Play the animation of creating the circle
      self.play(Create(c))

\`\`\`

The following is an example of the code with a voiceover:

\`\`\`
from manim import *
from manim_voiceover import VoiceoverScene
from manim_voiceover.services.gtts import GTTSService
from manim.scene.scene_file_writer import SceneFileWriter

# Disable the creation of subcaption files only when using Text-to-Speech
SceneFileWriter.write_subcaption_file = lambda *args, **kwargs: None

class GenScene(VoiceoverScene):
    def construct(self):
        # Set the voiceover service to Google Text-to-Speech
        self.set_speech_service(GTTSService())
        
        # Create a circle of color BLUE
        c = Circle(color=BLUE)
        c.move_to(ORIGIN)

        # Voiceover with animation
        with self.voiceover(
            text="This circle is drawn as I speak.",
            subcaption="What a cute circle! :)"
        ) as tracker:
            self.play(Create(c), run_time=tracker.duration)
\`\`\`

Let the user know that they can hear the audio of the voiceover by clicking on the "Animate" button.

Remember the part of `from manim.scene.scene_file_writer import SceneFileWriter` `SceneFileWriter.write_subcaption_file = lambda *args, **kwargs: None` because it's important to not create a subcaption file.

## Rules of programming

1. Always use comments to explain the next line of the code:

\`\`\`
# Create a sphere of color BLUE for the Earth
earth = Sphere(radius=1, checkerboard_colors=[BLUE_D, BLUE_E])
\`\`\`

This is needed to understand what you meant to do.

2. You can use TODO comments to mark places that you think can be improved, and you can come back later to them.

\`\`\`
# TODO: Add more colors to the cube later
\`\`\`

This is needed to understand what you could have done better.

3. Everytime there is a movement on the camera or on the objects, you should add a comment resalting the desired movement

\`\`\`
# With this movement we should see the difference between the both buildings
self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)
\`\`\`

This is needed to understand what you meant to reflect on the camera.

4. Unless described on this system prompt, always assume the user is not providing any image to you. That means, you should not use, for example:

\`\`\`
# Create a 3D grid of spheres
new_texture = "assets/random_texture.jpg"
\`\`\`

If `random_texture.jpg` is not provided, you should not use it. Otherwise the video will not work on the platform.

5. At the very end of all the thinking process, you should provide the final code to the user in a codeblock (without the ```python at the beginning and the end). And tell the user to click on "Animate" to see the video. Like this:

- ...Finally, you can hit "Animate" to render the video at the right side.
- ...Now to see the animation, click on "Animate".
- ...Cool, that's all! Now click on "Animate" to see the video.

That message should appear after the code, as the last message of the conversation.

6. If the user request a complex animation or an animation of a complex component, scene or object. Before generating the code, you need to think about it, these are some good examples to follow:

- User: I want to animate a car.
- Assistant (You): Okay, I need to generate Manim code for a car. Let me start by thinking about how to represent a car using Manim's shapes. A car typically has a body, wheels, windows, and maybe some details like headlights. Let's break it down into parts. First, the car body. Maybe a rectangle for the main part. Then a rounded rectangle or a polygon for the top part to make it look more like a car. I can use a Rectangle or RoundedRectangle class for the body. Maybe start with a rectangle and then add a trapezoid or something on top for the roof. Next, the wheels. Circles would work for wheels. I should place them at the bottom of the car body. Two circles on each side, maybe. To make them look like tires, perhaps add a smaller circle inside for the hubcap. Windows. The car probably has a front and rear window. Maybe two trapezoids or rectangles on top of the car body. Or combine them into a single shape with a line dividing them. Headlights and other details. Small circles or rectangles at the front and back.

- User: Create a Solar System.
- Assistant (You): Let's think about how to represent a Solar System using Manim's shapes. A Solar System typically has a Sun, planets, and maybe some details like moons. Let's break it down into parts. First, the Sun. Maybe a circle for the Sun. Then, the planets. Each planet is a circle, and they orbit around the Sun. Maybe start with the inner planets and then add the outer planets. Next, the moons. Each moon is a smaller circle that orbits around its planet. Also I should add a line to represent the orbit of each planet. The color of the planets should be different, and the color of the Sun should be yellow.

7. Always use `GenScene` as the class name, unless the user asks for a different name. Use `GenScene` as the class name by default.

## Rules of behaviour

1. If the user just says Hello, or something like that, you should not generate any code. Just tell the user you are ready to help them. Show what you can do, suggest what you can do.

2. Always be very kind, you can use words (in English, or translated to other languages) like:
- You're right!...
- That's awesome...
- Yes, I think I can do that!...
- Ah! That's true!, sorry...

3. If the user have a question you can't answer about Animo (the platform animo.video), you can tell them to send a message to team@animo.video with a clear description of the question or problem. Tell them we're back to them in less than 24 hours.

# Manim Library
{manimDocs}
"""

    messages.insert(0, {"role": "system", "content": general_system_prompt})

    if engine == "anthropic":
        client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

        def get_preview(code: str, class_name: str):
            """
            get_preview is a function that generates PNGs frames from a Manim script animation.
            
            IMPORTANT: This version of the function will only work for OpenAI models.
            """
            
            print("Generating preview")

            # Get the absolute path of the current script (in api/routes)
            current_dir = os.path.dirname(os.path.abspath(__file__))
            api_dir = os.path.dirname(current_dir)  # This should be the /api directory
            
            # Create the temporary directory inside /api
            temp_dir = os.path.join(api_dir, "temp_manim")
            os.makedirs(temp_dir, exist_ok=True)
            
            # Create the Python file in the temporary location
            file_name = f"{class_name}.py"
            file_path = os.path.join(temp_dir, file_name)
            
            preview_code = f"""
from manim import *
from math import *

{code}
            """
            
            with open(file_path, "w") as f:
                f.write(preview_code)
            
            # Run the Manim command
            command = f"manim {file_path} {class_name} --format=png --media_dir {temp_dir} --custom_folders -pql --disable_caching"
            try:
                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
                
                print(f"Result: {result}")
                
                # Create the previews directory if it doesn't exist
                previews_dir = os.path.join(api_dir, "public", "previews")
                os.makedirs(previews_dir, exist_ok=True)

                # Generate a random string for the subfolder
                random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=12))
                
                # Move the generated PNGs to the previews directory
                source_dir = temp_dir
                destination_dir = os.path.join(previews_dir, random_string, class_name)
                
                # Find all PNG files in the source directory
                png_files = [f for f in os.listdir(source_dir) if f.endswith('.png')]
                
                if png_files:
                    os.makedirs(destination_dir, exist_ok=True)
                    image_list = []
                    for png_file in png_files:
                        shutil.move(os.path.join(source_dir, png_file), os.path.join(destination_dir, png_file))
                        # Extract the index from the filename
                        match = re.search(r'(\d+)\.png$', png_file)
                        if match:
                            index = int(match.group(1))
                            if index % 4 == 0:  # Only include frames where index is divisible by 5
                                image_path = os.path.join(destination_dir, png_file)
                                with Image.open(image_path) as img:
                                    # Calculate new dimensions (half the original size)
                                    width, height = img.size
                                    new_width = width // 4
                                    new_height = height // 4
                                    # Resize the image
                                    resized_img = img.resize((new_width, new_height), Image.LANCZOS)
                                    # Save the resized image to a bytes buffer
                                    buffer = io.BytesIO()
                                    resized_img.save(buffer, format="PNG")
                                    # Get the base64 encoding of the resized image
                                    base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
                                image_list.append({
                                    "path": image_path,
                                    "index": index,
                                    "base64": base64_image
                                })
                    image_list.sort(key=lambda x: x["index"])
                    return json.dumps({
                        "message": f"Animation preview generated. Now you will see the image frames in the next automatic message...",
                        "images": image_list
                    })
                else:
                    print(f"No PNG files found in: {source_dir}")
                    return json.dumps({
                        "error": f"No preview files generated at expected location: {source_dir}",
                        "images": []
                    })
            except subprocess.CalledProcessError as e:
                error_output = e.stdout + e.stderr
                print(f"Error running Manim command: {str(e)}")
                print(f"Command output:\n{error_output}")
                return json.dumps({
                    "error": f"ERROR. Error generating preview, please think on what could be the problem, and use `get_preview` to run the code again: {str(e)}\nCommand output:\n{error_output}",
                    "images": []
                })
            except Exception as e:
                print(f"Unexpected error: {str(e)}")
                return json.dumps({
                    "error": f"Unexpected error: {str(e)}",
                    "images": []
                })

        def convert_message_for_anthropic(message):
            if isinstance(message["content"], list):
                content = []
                for part in message["content"]:
                    if part.get("type") == "image_url":
                        content.append(
                            {"type": "image", "image": part["image_url"]["url"]}
                        )
                    else:
                        content.append(part)
                message["content"] = content
            return message

        # Extract system message and remove it from the messages list
        system_message = next(
            (msg["content"] for msg in messages if msg["role"] == "system"), None
        )
        anthropic_messages = [
            convert_message_for_anthropic(msg)
            for msg in messages
            if msg["role"] != "system"
        ]

        def generate():
            try:
                messages = anthropic_messages
                while True:
                    print("\n=== Starting new message stream ===")
                    print("=== Current message history ===")
                    for idx, msg in enumerate(messages):
                        print(f"\nMessage {idx}:")
                        print(f"Role: {msg['role']}")
                        if isinstance(msg['content'], list):
                            print("Content (list):")
                            for content_item in msg['content']:
                                if isinstance(content_item, dict):
                                    print(f"  Type: {content_item.get('type', 'unknown')}")
                                    if content_item['type'] == 'text':
                                        print(f"  Text: {content_item['text']}")
                                    elif content_item['type'] == 'tool_result':
                                        print(f"  Tool use ID: {content_item.get('tool_use_id')}")
                        else:
                            print(f"Content: {msg['content']}")
                    print("\n=== End of message history ===")
                    
                    stream = client.messages.create(
                        model="claude-3-5-sonnet-20241022",
                        messages=messages,
                        system=system_message,
                        max_tokens=1000,
                        stream=True,
                        tools=animo_functions["anthropic"]
                    )
                    
                    current_message = {"role": "assistant", "content": []}
                    current_text = ""  # To accumulate text content
                    json_buffer = ""
                    should_continue = False
                    tool_use_id = None
                    complete_json = ""
                    
                    for chunk in stream:
                        print(f"\nChunk type: {chunk.type}")
                        print(f"Chunk content: {chunk}")
                        
                        if chunk.type == "content_block_start":
                            if hasattr(chunk.content_block, 'type'):
                                if chunk.content_block.type == 'tool_use':
                                    tool_use_id = chunk.content_block.id
                                    print(f"Captured tool_use_id: {tool_use_id}")
                                    
                                    # If we have accumulated text, add it first
                                    if current_text:
                                        current_message["content"].append({
                                            "type": "text",
                                            "text": current_text
                                        })
                                        current_text = ""
                                    
                                    # Add tool use block
                                    tool_input = {}
                                    if complete_json:
                                        try:
                                            tool_input = json.loads(complete_json)
                                        except json.JSONDecodeError:
                                            print("Failed to parse tool input JSON")
                                    
                                    current_message["content"].append({
                                        "type": "tool_use",
                                        "id": tool_use_id,
                                        "name": "get_preview",
                                        "input": tool_input
                                    })
                        
                        elif chunk.type == "content_block_delta":
                            if hasattr(chunk.delta, 'text'):
                                content = chunk.delta.text
                                if content:
                                    print(f"Text content: {content}")
                                    current_text += content  # Accumulate text
                                    if is_for_platform:
                                        for char in content:
                                            escaped_char = repr(char)[1:-1]
                                            yield f'0:"{escaped_char}"\n'
                                    else:
                                        yield content
                                
                            elif hasattr(chunk.delta, 'partial_json'):
                                complete_json += chunk.delta.partial_json
                                print(f"Accumulated complete JSON: {complete_json}")

                        elif chunk.type == "content_block_stop":
                            if complete_json:
                                try:
                                    print("\n=== Processing tool call ===")
                                    tool_call = json.loads(complete_json)
                                    print(f"Tool call data: {json.dumps(tool_call, indent=2)}")
                                    print(f"Using tool_use_id: {tool_use_id}")
                                    
                                    preview_result = get_preview(
                                        code=tool_call.get('code', ''),
                                        class_name=tool_call.get('class_name', '')
                                    )
                                    
                                    try:
                                        preview_data = json.loads(preview_result)
                                        print("\nParsed preview data keys:", preview_data.keys())
                                        
                                        # Just use the middle frame for testing
                                        middle_frame = preview_data['images'][len(preview_data['images'])//2]
                                        base64_data = middle_frame['base64']
                                        
                                        # Debug the base64 data
                                        print(f"\nBase64 data length: {len(base64_data)}")
                                        print(f"Base64 data starts with: {base64_data[:50]}")
                                        print(f"Base64 data ends with: {base64_data[-50:]}")
                                        
                                        # Create simplified content blocks with just one frame
                                        content_blocks = [
                                            {
                                                "type": "image",
                                                "source": {
                                                    "type": "base64",
                                                    "media_type": "image/png",
                                                    "data": base64_data  # Use raw base64 without prefix
                                                }
                                            },
                                            {
                                                "type": "text",
                                                "text": f"\nPreview frame from the animation.\n"
                                            }
                                        ]
                                        
                                        tool_response = {
                                            "role": "user",
                                            "content": [{
                                                "type": "tool_result",
                                                "tool_use_id": tool_use_id,
                                                "content": content_blocks
                                            }]
                                        }
                                        
                                        print(f"\nTool response structure:")
                                        print(json.dumps(tool_response, indent=2))
                                        
                                        # Add the assistant's message with tool use before adding tool result
                                        messages.append(current_message)
                                        messages.append(tool_response)
                                        should_continue = True
                                        
                                        preview_text = "Generated preview of the animation:\n"
                                        if is_for_platform:
                                            for char in preview_text:
                                                escaped_char = repr(char)[1:-1]
                                                yield f'0:"{escaped_char}"\n'
                                            yield f'0:"[IMAGE: Preview frame]"\n'
                                        else:
                                            yield "\n[Preview frame]\n"
                                        
                                    except json.JSONDecodeError:
                                        print("Failed to parse preview result as JSON")
                                        print("Raw preview result:", preview_result)
                                        continue
                                
                                except Exception as e:
                                    print(f"Error processing tool call: {str(e)}")
                                    continue
                        
                        elif chunk.type == "message_stop":
                            print("\n=== Message stream ended ===")
                            # Add any remaining text content
                            if current_text:
                                if not current_message["content"]:
                                    current_message["content"] = []
                                current_message["content"].append({
                                    "type": "text",
                                    "text": current_text
                                })
                                messages.append(current_message)
                            
                            if not should_continue:
                                return
                            break
                    
                    if not should_continue:
                        break

            except Exception as e:
                print(f"\n=== Error occurred ===\nError details: {str(e)}")
                error_message = f'0:"{str(e)}"\n' if is_for_platform else f"Error: {str(e)}"
                yield error_message

        response = Response(stream_with_context(generate()), content_type="text/plain; charset=utf-8" if is_for_platform else "text/event-stream")
        if is_for_platform:
            response.headers['Transfer-Encoding'] = 'chunked'
            response.headers['x-vercel-ai-data-stream'] = 'v1'
        return response

    else:
        client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
                
        def get_preview(code: str, class_name: str):
            """
            get_preview is a function that generates PNGs frames from a Manim script animation.
            
            IMPORTANT: This version of the function will only work for OpenAI models.
            """
            
            print("Generating preview")

            # Get the absolute path of the current script (in api/routes)
            current_dir = os.path.dirname(os.path.abspath(__file__))
            api_dir = os.path.dirname(current_dir)  # This should be the /api directory
            
            # Create the temporary directory inside /api
            temp_dir = os.path.join(api_dir, "temp_manim")
            os.makedirs(temp_dir, exist_ok=True)
            
            # Create the Python file in the temporary location
            file_name = f"{class_name}.py"
            file_path = os.path.join(temp_dir, file_name)
            
            preview_code = f"""
from manim import *
from math import *

{code}
            """
            
            with open(file_path, "w") as f:
                f.write(preview_code)
            
            # Run the Manim command
            command = f"manim {file_path} {class_name} --format=png --media_dir {temp_dir} --custom_folders -pql --disable_caching"
            try:
                result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
                
                print(f"Result: {result}")
                
                # Create the previews directory if it doesn't exist
                previews_dir = os.path.join(api_dir, "public", "previews")
                os.makedirs(previews_dir, exist_ok=True)

                # Generate a random string for the subfolder
                random_string = ''.join(random.choices(string.ascii_letters + string.digits, k=12))
                
                # Move the generated PNGs to the previews directory
                source_dir = temp_dir
                destination_dir = os.path.join(previews_dir, random_string, class_name)
                
                # Find all PNG files in the source directory
                png_files = [f for f in os.listdir(source_dir) if f.endswith('.png')]
                
                if png_files:
                    os.makedirs(destination_dir, exist_ok=True)
                    image_list = []
                    for png_file in png_files:
                        shutil.move(os.path.join(source_dir, png_file), os.path.join(destination_dir, png_file))
                        # Extract the index from the filename
                        match = re.search(r'(\d+)\.png$', png_file)
                        if match:
                            index = int(match.group(1))
                            if index % 4 == 0:  # Only include frames where index is divisible by 5
                                image_path = os.path.join(destination_dir, png_file)
                                with Image.open(image_path) as img:
                                    # Calculate new dimensions (half the original size)
                                    width, height = img.size
                                    new_width = width // 4
                                    new_height = height // 4
                                    # Resize the image
                                    resized_img = img.resize((new_width, new_height), Image.LANCZOS)
                                    # Save the resized image to a bytes buffer
                                    buffer = io.BytesIO()
                                    resized_img.save(buffer, format="PNG")
                                    # Get the base64 encoding of the resized image
                                    base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
                                image_list.append({
                                    "path": image_path,
                                    "index": index,
                                    "base64": base64_image
                                })
                    image_list.sort(key=lambda x: x["index"])
                    return json.dumps({
                        "message": f"Animation preview generated. Now you will see the image frames in the next automatic message...",
                        "images": image_list
                    })
                else:
                    print(f"No PNG files found in: {source_dir}")
                    return json.dumps({
                        "error": f"No preview files generated at expected location: {source_dir}",
                        "images": []
                    })
            except subprocess.CalledProcessError as e:
                error_output = e.stdout + e.stderr
                print(f"Error running Manim command: {str(e)}")
                print(f"Command output:\n{error_output}")
                return json.dumps({
                    "error": f"ERROR. Error generating preview, please think on what could be the problem, and use `get_preview` to run the code again: {str(e)}\nCommand output:\n{error_output}",
                    "images": []
                })
            except Exception as e:
                print(f"Unexpected error: {str(e)}")
                return json.dumps({
                    "error": f"Unexpected error: {str(e)}",
                    "images": []
                })

        def generate():
            max_retries = 3
            retry_delay = 4  # seconds

            while True:
                for attempt in range(max_retries):
                    try:
                        stream = client.chat.completions.create(
                            model="gpt-4o",
                            messages=messages,
                            stream=True,
                            functions=animo_functions["openai"],
                            function_call="auto",
                        )
                        function_call_data = ""
                        function_name = ""
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                if is_for_platform:
                                    text_obj = json.dumps({"type": "text", "text": content})
                                    yield f'{text_obj}\n'
                                else:
                                    yield content
                            elif chunk.choices[0].delta.function_call:
                                if chunk.choices[0].delta.function_call.name:
                                    function_name = chunk.choices[0].delta.function_call.name
                                    if is_for_platform:
                                        initial_call_obj = json.dumps({
                                            "type": "function_call",
                                            "content": "",
                                            "function_call": {"name": function_name}
                                        })
                                        yield f'{initial_call_obj}\n'
                                if chunk.choices[0].delta.function_call.arguments:
                                    chunk_data = chunk.choices[0].delta.function_call.arguments
                                    function_call_data += chunk_data
                                    if is_for_platform:
                                        partial_call_obj = json.dumps({
                                            "type": "function_call",
                                            "content": "",
                                            "function_call": {"args": chunk_data}
                                        })
                                        yield f'{partial_call_obj}\n'
                        
                        # If we get here, the stream completed successfully
                        break
                    
                    except APIError as e:
                        if attempt < max_retries - 1:
                            print(f"APIError occurred: {str(e)}. Retrying in {retry_delay} seconds...")
                            time.sleep(retry_delay)
                        else:
                            print(f"Max retries reached. APIError: {str(e)}")
                            yield json.dumps({"error": "Max retries reached due to API errors"})
                            return  # Exit the generator

                if function_call_data:
                    # Add the function call to messages
                    messages.append({
                        "role": "assistant",
                        "content": None,
                        "function_call": {
                            "name": function_name,
                            "arguments": function_call_data
                        }
                    })

                    # Yield the whole object back to the frontend
                    function_call_obj = json.dumps({
                        "role": "assistant",
                        "content": None,
                        "function_call": {
                            "name": function_name,
                            "arguments": function_call_data
                        }
                    })
                    if is_for_platform:
                        pass
                        # text_obj = json.dumps({"type": "text", "text": function_call_obj})
                        # yield f'{text_obj}\n'
                    else:
                        pass

                    # Actually call get_preview
                    if function_name == "get_preview":
                        print(f"Calling get_preview with data: {function_call_data}")
                        args = json.loads(function_call_data)
                        result = get_preview(args['code'], args['class_name'])
                        result_json = json.loads(result)
                        function_response = {
                            "content": result_json.get("message", result_json.get("error")),
                            "name": "get_preview",
                            "role": "function"
                        }
                        messages.append(function_response)

                        # Yield the function response back to the frontend
                        if is_for_platform:
                            function_result_obj = json.dumps({
                                "type": "function_result",
                                "content": function_response,
                                "function_call": {"name": function_name}
                            })
                            yield f'{function_result_obj}\n'
                        else:
                            pass

                        # Only create and send image_message if there are images
                        if result_json.get("images"):
                            # Create a new message with the images
                            image_message = {
                                "role": "user",
                                "content": [
                                    {
                                        "type": "text",
                                        "text": """ASSISTANT_MESSAGE_PREVIEW_GENERATED: This message is not generated by the user, but automatically by you, the assistant when firing the `get_preview` function, this message might not be visible to the user.
                                        
                                        The following images are selected frames of the animation generated. Please check these frames and follow the rules: Text should not be overlapping, the space should be used efficiently, use different colors to represent different objects, plus other improvements you can think of.
                                        
                                        You can decide now if you want to iterate on the animation (if it's too complex), or just stop here and provide the final code to the user now."""
                                    }
                                ]
                            }

                            # Calculate how many new images we can add
                            available_slots = manage_conversation_images(messages, len(result_json["images"]), engine)
                            
                            # Select frames based on available slots
                            total_frames = len(result_json["images"])
                            frame_interval = max(1, total_frames // available_slots)
                            selected_frames = result_json["images"][::frame_interval][:available_slots]

                            for image in selected_frames:
                                image_message["content"].append({
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{image['base64']}"
                                    }
                                })
                            messages.append(image_message)

                            # Yield the image message back to the frontend
                            image_message_obj = json.dumps(image_message)
                            if is_for_platform:
                                pass
                                # text_obj = json.dumps({"type": "text", "text": image_message_obj})
                                # yield f'{text_obj}\n'
                            else:
                                yield image_message_obj

                        # Trigger a new response from the assistant
                        continue  # This will start a new iteration of the while loop
                    else:
                        break  # Exit the loop if it's not a get_preview function call
                else:
                    break  # Exit the loop if there's no function call

            # Final message when there are no more function calls
            final_message = "\n"
            if is_for_platform:
                text_obj = json.dumps({"type": "text", "text": final_message})
                yield f'{text_obj}\n'
            else:
                yield final_message

        print("Generating response")
        response = Response(stream_with_context(generate()), content_type="text/plain; charset=utf-8")
        if is_for_platform:
            response.headers['Transfer-Encoding'] = 'chunked'
            response.headers['x-vercel-ai-data-stream'] = 'v1'
        return response



================================================
FILE: api/routes/code_generation.py
================================================
from flask import Blueprint, jsonify, request
import anthropic
import os
from openai import OpenAI

code_generation_bp = Blueprint('code_generation', __name__)

@code_generation_bp.route('/v1/code/generation', methods=['POST'])
def generate_code():
    body = request.json
    prompt_content = body.get("prompt", "")
    model = body.get("model", "gpt-4o")

    general_system_prompt = """
You are an assistant that knows about Manim. Manim is a mathematical animation engine that is used to create videos programmatically.

The following is an example of the code:
\`\`\`
from manim import *
from math import *

class GenScene(Scene):
def construct(self):
    c = Circle(color=BLUE)
    self.play(Create(c))

\`\`\`

# Rules
1. Always use GenScene as the class name, otherwise, the code will not work.
2. Always use self.play() to play the animation, otherwise, the code will not work.
3. Do not use text to explain the code, only the code.
4. Do not explain the code, only the code.
    """

    if model.startswith("claude-"):
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        messages = [{"role": "user", "content": prompt_content}]
        try:
            response = client.messages.create(
                model=model,
                max_tokens=1000,
                temperature=0.2,
                system=general_system_prompt,
                messages=messages,
            )

            # Extract the text content from the response
            code = "".join(block.text for block in response.content)

            return jsonify({"code": code})

        except Exception as e:
            return jsonify({"error": str(e)}), 500

    else:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        messages = [
            {"role": "system", "content": general_system_prompt},
            {"role": "user", "content": prompt_content},
        ]

        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
            )

            code = response.choices[0].message.content

            return jsonify({"code": code})

        except Exception as e:
            return jsonify({"error": str(e)}), 500



================================================
FILE: api/routes/video_rendering.py
================================================
from flask import Blueprint, jsonify, current_app, request, Response
import subprocess
import os
import re
import json
import sys
import traceback
from azure.storage.blob import BlobServiceClient
import shutil
from typing import Union
import uuid
import time
import requests

video_rendering_bp = Blueprint("video_rendering", __name__)


USE_LOCAL_STORAGE = os.getenv("USE_LOCAL_STORAGE", "true") == "true"
BASE_URL = os.getenv("BASE_URL", "http://127.0.0.1:8080")


def upload_to_azure_storage(file_path: str, video_storage_file_name: str) -> str:
    """
    Uploads the video to Azure Blob Storage and returns the URL.
    """
    cloud_file_name = f"{video_storage_file_name}.mp4"

    connection_string = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
    container_name = os.getenv("AZURE_STORAGE_CONTAINER_NAME")
    blob_service_client = BlobServiceClient.from_connection_string(connection_string)
    blob_client = blob_service_client.get_blob_client(
        container=container_name, blob=cloud_file_name
    )

    # Upload the video file
    with open(file_path, "rb") as data:
        blob_client.upload_blob(data, overwrite=True)

    # Construct the URL of the uploaded blob
    blob_url = f"https://{blob_service_client.account_name}.blob.core.windows.net/{container_name}/{cloud_file_name}"
    return blob_url


def move_to_public_folder(
    file_path: str, video_storage_file_name: str, base_url: Union[str, None] = None
) -> str:
    """
    Moves the video to the public folder and returns the URL.
    """
    public_folder = os.path.join(os.path.dirname(__file__), "public")
    os.makedirs(public_folder, exist_ok=True)

    new_file_name = f"{video_storage_file_name}.mp4"
    new_file_path = os.path.join(public_folder, new_file_name)

    shutil.move(file_path, new_file_path)

    # Use the provided base_url if available, otherwise fall back to BASE_URL
    url_base = base_url if base_url else BASE_URL
    video_url = f"{url_base.rstrip('/')}/public/{new_file_name}"
    return video_url


def get_frame_config(aspect_ratio):
    if aspect_ratio == "16:9":
        return (3840, 2160), 14.22
    elif aspect_ratio == "9:16":
        return (1080, 1920), 8.0
    elif aspect_ratio == "1:1":
        return (1080, 1080), 8.0
    else:
        return (3840, 2160), 14.22


@video_rendering_bp.route("/v1/video/rendering", methods=["POST"])
def render_video():
    # Get the API key from the request headers
    # api_key = request.headers.get('X-API-Key')
    
    # if not api_key:
    #     return jsonify({"error": "API key is missing"}), 401
    
    # Validate the API key and get the user ID
    # user_id = get_user_by_api_key(api_key)
    
    # if not user_id:
    #     return jsonify({"error": "Invalid API key"}), 401
    
    # Now that we have a valid user_id, create a run
    # run_id = create_run_on_user(user_id, "video")
    
    # Extract the rest of the request data
    code = request.json.get("code")
    file_name = request.json.get("file_name")
    file_class = request.json.get("file_class")

    user_id = request.json.get("user_id") or str(uuid.uuid4())
    project_name = request.json.get("project_name")
    iteration = request.json.get("iteration")

    # Aspect Ratio can be: "16:9" (default), "1:1", "9:16"
    aspect_ratio = request.json.get("aspect_ratio")

    # Stream the percentage of animation it shown in the error
    stream = request.json.get("stream", False)

    video_storage_file_name = f"video-{user_id}-{project_name}-{iteration}"

    if not code:
        return jsonify(error="No code provided"), 400

    # Determine frame size and width based on aspect ratio
    frame_size, frame_width = get_frame_config(aspect_ratio)

    # Modify the Manim script to include configuration settings
    modified_code = f"""
from manim import *
from math import *
config.frame_size = {frame_size}
config.frame_width = {frame_width}

{code}
    """

    # Create a unique file name
    file_name = f"scene_{os.urandom(2).hex()}.py"
    
    # Adjust the path to point to /api/public/
    api_dir = os.path.dirname(os.path.dirname(__file__))  # Go up one level from routes
    public_dir = os.path.join(api_dir, "public")
    os.makedirs(public_dir, exist_ok=True)  # Ensure the public directory exists
    file_path = os.path.join(public_dir, file_name)

    # Write the code to the file
    with open(file_path, "w") as f:
        f.write(modified_code)

    def render_video():
        try:
            command_list = [
                "manim",
                file_path,  # Use the full path to the file
                file_class,
                "--format=mp4",
                "--media_dir",
                ".",
                "--custom_folders",
            ]

            process = subprocess.Popen(
                command_list,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=os.path.dirname(os.path.realpath(__file__)),
                text=True,
                bufsize=1,  # Ensure the output is in text mode and line-buffered
            )
            current_animation = -1
            current_percentage = 0
            error_output = []
            in_error = False

            while True:
                output = process.stdout.readline()
                error = process.stderr.readline()

                if output == "" and error == "" and process.poll() is not None:
                    break

                if output:
                    print("STDOUT:", output.strip())
                if error:
                    print("STDERR:", error.strip())
                    error_output.append(error.strip())
                    
                # Check for critical errors
                if "is not in the script" in error:
                    in_error = True
                    continue

                # Check for start of error
                if "Traceback (most recent call last)" in error:
                    in_error = True
                    continue

                # If we're in an error state, keep accumulating the error message
                if in_error:
                    if error.strip() == "":
                        # Empty line might indicate end of traceback
                        in_error = False
                        full_error = "\n".join(error_output)
                        yield f'{{"error": {json.dumps(full_error)}}}\n'
                        return
                    continue

                animation_match = re.search(r"Animation (\d+):", error)
                if animation_match:
                    new_animation = int(animation_match.group(1))
                    if new_animation != current_animation:
                        current_animation = new_animation
                        current_percentage = 0
                        yield f'{{"animationIndex": {current_animation}, "percentage": 0}}\n'

                percentage_match = re.search(r"(\d+)%", error)
                if percentage_match:
                    new_percentage = int(percentage_match.group(1))
                    if new_percentage != current_percentage:
                        current_percentage = new_percentage
                        yield f'{{"animationIndex": {current_animation}, "percentage": {current_percentage}}}\n'

            if process.returncode == 0:
                # Update this part
                video_file_path = os.path.join(
                    os.path.dirname(os.path.realpath(__file__)),
                    f"{file_class or 'GenScene'}.mp4"
                )
                # Looking for video file at: {video_file_path}
                
                if not os.path.exists(video_file_path):
                    #  Video file not found. Searching in parent directory...
                    video_file_path = os.path.join(
                        os.path.dirname(os.path.dirname(os.path.realpath(__file__))),
                        f"{file_class or 'GenScene'}.mp4"
                    )
                    # New video file path is: {video_file_path}

                if os.path.exists(video_file_path):
                    print(f"Video file found at: {video_file_path}")
                else:
                    print(f"Video file not found. Files in current directory: {os.listdir(os.path.dirname(video_file_path))}")
                    raise FileNotFoundError(f"Video file not found at {video_file_path}")

                print(f"Files in video file directory: {os.listdir(os.path.dirname(video_file_path))}")
                
                if USE_LOCAL_STORAGE:
                    # Pass request.host_url if available
                    base_url = (
                        request.host_url
                        if request and hasattr(request, "host_url")
                        else None
                    )
                    video_url = move_to_public_folder(
                        video_file_path, video_storage_file_name, base_url
                    )
                else:
                    video_url = upload_to_azure_storage(
                        video_file_path, video_storage_file_name
                    )
                print(f"Video URL: {video_url}")
                if stream:
                    yield f'{{ "video_url": "{video_url}" }}\n'
                    sys.stdout.flush()
                else:
                    yield {
                        "message": "Video generation completed",
                        "video_url": video_url,
                    }
            else:
                full_error = "\n".join(error_output)
                yield f'{{"error": {json.dumps(full_error)}}}\n'

        except Exception as e:
            print(f"Unexpected error: {str(e)}")
            traceback.print_exc()
            print(f"Files in current directory after error: {os.listdir('.')}")
            yield f'{{"error": "Unexpected error occurred: {str(e)}"}}\n'
        finally:
            # Remove the temporary Python file
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"Removed temporary file: {file_path}")
                # Remove the video file
                if os.path.exists(video_file_path):
                    os.remove(video_file_path)
                    print(f"Removed temporary video file: {video_file_path}")
            except Exception as e:
                print(f"Error removing temporary file {file_path}: {e}")

    if stream:
        # TODO: If the `render_video()` fails, or it's sending {"error"}, be sure to add `500`
        return Response(
            render_video(), content_type="text/event-stream", status=207
        )
    else:
        video_url = None
        try:
            for result in render_video():  # Iterate through the generator
                print(f"Generated result: {result}")  # Debug print
                if isinstance(result, dict):
                    if "video_url" in result:
                        video_url = result["video_url"]
                    elif "error" in result:
                        raise Exception(result["error"])

            if video_url:
                return (
                    jsonify(
                        {
                            "message": "Video generation completed",
                            "video_url": video_url,
                        }
                    ),
                    200,
                )
            else:
                return (
                    jsonify(
                        {
                            "message": "Video generation completed, but no URL was found"
                        }
                    ),
                    200,
                )
        except StopIteration:
            if video_url:
                return (
                    jsonify(
                        {
                            "message": "Video generation completed",
                            "video_url": video_url,
                        }
                    ),
                    200,
                )
            else:
                return (
                    jsonify(
                        {
                            "message": "Video generation completed, but no URL was found"
                        }
                    ),
                    200,
                )
        except Exception as e:
            print(f"Error in non-streaming mode: {e}")
            return jsonify({"error": str(e)}), 500


@video_rendering_bp.route("/v1/video/exporting", methods=["POST"])
def export_video():
    scenes = request.json.get("scenes")
    title_slug = request.json.get("titleSlug")
    local_filenames = []

    # Download each scene
    for scene in scenes:
        video_url = scene["videoUrl"]
        object_name = video_url.split("/")[-1]
        local_filename = download_video(video_url)
        local_filenames.append(local_filename)

    # Create a list of input file arguments for ffmpeg
    input_files = " ".join([f"-i {filename}" for filename in local_filenames])

    # Generate a unique filename with UNIX timestamp
    timestamp = int(time.time())
    merged_filename = os.path.join(
        os.getcwd(), f"exported-scene-{title_slug}-{timestamp}.mp4"
    )

    # Command to merge videos using ffmpeg
    command = f"ffmpeg {input_files} -filter_complex 'concat=n={len(local_filenames)}:v=1:a=0[out]' -map '[out]' {merged_filename}"

    try:
        # Execute the ffmpeg command
        subprocess.run(command, shell=True, check=True)
        print("Videos merged successfully.")
        print(f"merged_filename: {merged_filename}")
        public_url = upload_to_azure_storage(
            merged_filename, f"exported-scene-{title_slug}-{timestamp}"
        )
        print(f"Video URL: {public_url}")
        return jsonify(
            {"status": "Videos merged successfully", "video_url": public_url}
        )
    except subprocess.CalledProcessError as e:
        print(f"ffmpeg error: {e}")
        return jsonify({"error": "Failed to merge videos"}), 500
    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"error": str(e)}), 500


def download_video(video_url):
    local_filename = video_url.split("/")[-1]
    response = requests.get(video_url)
    response.raise_for_status()
    with open(local_filename, 'wb') as f:
        f.write(response.content)
    return local_filename



================================================
FILE: datasets/README.md
================================================
# Generative Manim Datasets & Data Collection Pipeline

Some of the techniques to create better prompt-to-code Manim models will need a guide for training. In order to achieve that we need to compile a dataset of prompts and the corresponding code.

## Sources

### Manim (Community)

- [Examples Gallery](https://docs.manim.community/en/stable/examples.html)

### Manim

- [Quickstart](https://3b1b.github.io/manim/getting_started/quickstart.html)
- [Example Scenes](https://3b1b.github.io/manim/getting_started/example_scenes.html#graphexample)

## Datasets

### Custom Dataset

Now, the structure we need to follow is to create a dataset with the following columns:
- `prompt`: Prompt to generate the code.
- `code`: Corresponding code.
- `type`: Type of media (`video`, `image`).

Altough we are focused on video generation, we should also consider images as a type of media, in order to train the model with vast examples that can be used in different scenarios.

- [x] Extract code examples from the Manim community.
- [ ] Tag each code example with the corresponding type of media (if it uses `self.add`, it is an image, if it uses `self.play`, it is a video).
- [ ] Write a prompt for each code example.

#### Dataset Generation Pipeline

> ğŸ’¡ Using the code as the prompt word to generate the prompt text, in other words: let GPT summarize the manim code, the quality is better

Instead of relying on humans to write the prompt, we can also generate the prompt from the code itself via GPT models. This way we can have a more consistent dataset.

- [ ] Create a Python script to generate the prompt from the code available in the scripts of `/code`.
- [ ] Create a JSONL file with the dataset generated.

### Dataset from `Edoh`

We can also use the dataset from `Edoh` to create a dataset of prompts and code.

- [x] Create Python script to extract the dataset from `Edoh` dataset.
- [x] Create JSONL file with the dataset.

Extracting the dataset of prompts and code from the [Edoh Manim Python](https://huggingface.co/datasets/Edoh/manim_python) dataset we got `edoh-dataset.jsonl`.

The dataset contains 599 examples of prompts and code.

```json
{"messages": [{"role": "system", "content": "Write Manim scripts for animations in Python. Generate code, not text."}, {"role": "user", "content": "Create a new scene named 'MyScene'."}, {"role": "assistant", "content": "from manim import * class MyScene(Scene): def construct(self): pass"}]}
{"messages": [{"role": "system", "content": "Write Manim scripts for animations in Python. Generate code, not text."}, {"role": "user", "content": "Add a circle with radius 2 and center at the origin to the scene."}, {"role": "assistant", "content": "from manim import * class MyScene(Scene): def construct(self): circle = Circle(radius=2, color=BLUE) self.add(circle)"}]}
{"messages": [{"role": "system", "content": "Write Manim scripts for animations in Python. Generate code, not text."}, {"role": "user", "content": "Create a text object saying 'Hello, World!' and add it to the scene."}, {"role": "assistant", "content": "from manim import * class MyScene(Scene): def construct(self): text = Text('Hello, World!') self.add(text)"}]}
```



================================================
FILE: datasets/physics-01.jsonl
================================================
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        charge1 = Charge(-1, LEFT + DOWN)\n        charge2 = Charge(2, RIGHT + DOWN)\n        charge3 = Charge(-1, UP)\n        field = ElectricField(charge1, charge2, charge3)\n\n        self.play(Create(charge1), Create(charge2), Create(charge3))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": BLUE}\n        a = Lens(-5, 1, **lens_style).shift(LEFT)\n        a2 = Lens(5, 1, **lens_style).shift(RIGHT)\n        b = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [a, a2], color=RED)\n            for i in np.linspace(-2, 2, 10)\n        ]\n\n        self.play(Create(a), Create(a2))\n        for ray in b:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP)\n        circle.set_fill(RED, 1)\n        circle.shift(DOWN + RIGHT)\n\n        rect = Square().shift(UP)\n        rect.rotate(PI / 4)\n        rect.set_fill(YELLOW_A, 1)\n        rect.shift(UP * 2)\n        rect.scale(0.5)\n\n        ground = Line([-4, -3.5, 0], [4, -3.5, 0])\n        wall1 = Line([-4, -3.5, 0], [-4, 3.5, 0])\n        wall2 = Line([4, -3.5, 0], [4, 3.5, 0])\n        walls = VGroup(ground, wall1, wall2)\n        self.add(walls)\n\n        self.play(\n            DrawBorderThenFill(circle),\n            DrawBorderThenFill(rect),\n        )\n        self.make_rigid_body(rect, circle)\n        self.make_static_body(walls)\n        self.wait(5)"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video of a double pendulum with orange balls and white rods on a black background. Animate the pendulum swinging in a chaotic motion, and show blue lines tracing its path."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        p = MultiPendulum(RIGHT, LEFT)\n        self.add(p)\n        self.make_rigid_body(*p.bobs)\n        p.start_swinging()\n        self.add(TracedPath(p.bobs[-1].get_center, stroke_color=BLUE))\n        self.wait(10)"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video of a pendulum with multiple orange balls connected by white rods on a black background. Animate the pendulum swinging back and forth, showing the balls moving in a synchronized wave-like motion"}, {"role": "assistant", "content": "\nfrom manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        pends = VGroup(*[Pendulum(i) for i in np.linspace(1, 5, 7)])\n        self.add(pends)\n        for p in pends:\n            self.Generate_rigid_body(*p.bobs)\n            p.start_swinging()\n        self.wait(10)"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing a 3D grid of blue waves moving in a synchronized pattern on a black background. Animate the waves to create a smooth, undulating motion across the grid."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(ThreeDScene):\n    def construct(self):\n        self.set_camera_orientation(60 * DEGREES, -45 * DEGREES)\n        wave = LinearWave()\n        self.add(wave)\n        wave.start_wave()\n        self.wait(3)\n        wave.stop_wave()"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing a 3D grid of blue waves moving on a black background. Animate the waves to create a dynamic, undulating motion across the surface."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(ThreeDScene):\n    def construct(self):\n        self.set_camera_orientation(60 * DEGREES, -45 * DEGREES)\n        wave = RadialWave(\n            LEFT * 2 + DOWN * 5,  # Dos fuentes de ondas\n            RIGHT * 2 + DOWN * 5,\n            checkerboard_colors=[BLUE_D],\n            stroke_width=0,\n        )\n        self.add(wave)\n        wave.start_wave()\n        self.wait(3)\n        wave.stop_wave()"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing 4 standing waves with different amplitudes on a black background. Animate the waves to create a smooth, undulating motion across the screen."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        wave1 = StandingWave(1)\n        wave2 = StandingWave(2)\n        wave3 = StandingWave(3)\n        wave4 = StandingWave(4)\n        waves = VGroup(wave1, wave2, wave3, wave4)\n        waves.arrange(DOWN).move_to(ORIGIN)\n        self.add(waves)\n        for wave in waves:\n            wave.start_wave()\n        self.wait()"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 2, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 2, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time. But make the circle fall at the left side of the box."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + LEFT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + RIGHT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 2, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 2, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 3, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 3, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": BLUE}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=RED)\n            for i in np.linspace(-2, 2, 10)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": GREEN}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=YELLOW)\n            for i in np.linspace(-2, 2, 10)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + LEFT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + RIGHT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + RIGHT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + LEFT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 2, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 2, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "I need a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 3, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 3, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "I need a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": BLUE}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=RED)\n            for i in np.linspace(-2, 2, 10)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "I need a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": GREEN}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=YELLOW)\n            for i in np.linspace(-2, 2, 10)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + LEFT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + RIGHT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + RIGHT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + LEFT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 2.5, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 2.5, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 3.5, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 3.5, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": BLUE}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=RED)\n            for i in np.linspace(-2, 2, 12)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": GREEN}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=YELLOW)\n            for i in np.linspace(-2, 2, 8)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + LEFT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + RIGHT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Make a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + RIGHT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + LEFT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 2.5, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 2.5, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video of an electromagnetic field with a central red positive charge and two blue negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=RED)\n        neg_charge1 = Charge(-1, LEFT * 3.5, color=BLUE)\n        neg_charge2 = Charge(-1, RIGHT * 3.5, color=BLUE)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": BLUE}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=RED)\n            for i in np.linspace(-2, 2, 12)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": GREEN}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=YELLOW)\n            for i in np.linspace(-2, 2, 8)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + LEFT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + RIGHT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing a YELLOW_A diamond on an orange circle inside a white box on a black background. Animate the diamond to turn yellow and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(ORANGE, 1).shift(DOWN + RIGHT)\n        diamond = Square().rotate(PI / 4).set_fill(YELLOW_A, 1).shift(UP * 2 + LEFT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(WHITE, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of an electromagnetic field with a central green positive charge and two yellow negative charges. Show arrows pointing away from the positive charge and towards the negative charges on a black background."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        pos_charge = Charge(1, ORIGIN, color=GREEN)\n        neg_charge1 = Charge(-1, LEFT * 2, color=YELLOW)\n        neg_charge2 = Charge(-1, RIGHT * 2, color=YELLOW)\n        field = ElectricField(pos_charge, neg_charge1, neg_charge2)\n\n        self.play(Create(pos_charge), Create(neg_charge1), Create(neg_charge2))\n        self.play(Create(field))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing light rays passing through a convex and a concave lens on a black background. Show how the rays bend as they pass through the lenses. Use purple for the rays and yellow for the lenses."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(Scene):\n    def construct(self):\n        lens_style = {\"fill_opacity\": 0.5, \"color\": YELLOW}\n        convex_lens = Lens(-5, 1, **lens_style).shift(LEFT)\n        concave_lens = Lens(5, 1, **lens_style).shift(RIGHT)\n        rays = [\n            Ray(LEFT * 5 + UP * i, RIGHT, 8, [convex_lens, concave_lens], color=PURPLE)\n            for i in np.linspace(-2, 2, 10)\n        ]\n\n        self.play(Create(convex_lens), Create(concave_lens))\n        for ray in rays:\n            self.play(Create(ray))"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video showing a green diamond on a blue circle inside a red box on a black background. Animate the diamond to turn green and the circle to fill with color, then have them fall at the same time."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        circle = Circle().shift(UP).set_fill(BLUE, 1).shift(DOWN + LEFT)\n        diamond = Square().rotate(PI / 4).set_fill(GREEN, 1).shift(UP * 2 + RIGHT).scale(0.6)\n        box = Rectangle(width=4, height=4).shift(DOWN * 2).set_stroke(RED, 3)\n\n        self.add(box)\n        self.play(DrawBorderThenFill(circle), DrawBorderThenFill(diamond))\n        self.make_rigid_body(circle, diamond)\n        self.make_static_body(box)\n        self.wait(5)\n"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Create a video of a double pendulum with green balls and red rods on a black background. Animate the pendulum swinging in a chaotic motion, and show yellow lines tracing its path."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        p = MultiPendulum(RIGHT, LEFT, bob_style={'color': GREEN}, rod_style={'color': RED})\n        self.add(p)\n        self.make_rigid_body(*p.bobs)\n        p.start_swinging()\n        self.add(TracedPath(p.bobs[-1].get_center, stroke_color=YELLOW))\n        self.wait(10)"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video of a pendulum with multiple green balls connected by red rods on a black background. Animate the pendulum swinging back and forth, showing the balls moving in a synchronized wave-like motion."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(SpaceScene):\n    def construct(self):\n        pends = VGroup(*[Pendulum(i, bob_style={'color': GREEN}, rod_style={'color': RED}) for i in np.linspace(1, 5, 7)])\n        self.add(pends)\n        for p in pends:\n            self.make_rigid_body(*p.bobs)\n            p.start_swinging()\n        self.wait(10)"}]}
{"messages": [{"role": "system", "content": "Create Manim scripts for animations in Python. Generate code, not text. You can also include physics simulations using the classes provided by `manim_physics`. The available classes for physics simulations are: `Charge`, `ElectricField`, `MagneticField`, `Wire`, `Lens`, `Ray`, `Space`, `SpaceScene`, `MultiPendulum`, `Pendulum`, `LinearWave`, `RadialWave`, and `StandingWave`."}, {"role": "user", "content": "Generate a video showing a 3D grid of red waves moving in a synchronized pattern on a black background. Animate the waves to create a smooth, undulating motion across the grid."}, {"role": "assistant", "content": "from manim import *\nfrom manim_physics import *\n\nclass GenScene(ThreeDScene):\n    def construct(self):\n        self.set_camera_orientation(60 * DEGREES, -45 * DEGREES)\n        wave = LinearWave(color=RED)\n        self.add(wave)\n        wave.start_wave()\n        self.wait(3)\n        wave.stop_wave()"}]}


================================================
FILE: datasets/code/ArgMinExample.md
================================================
Create a 5-second video illustrating the concept of argmin. Show the function f(x) = 2(x - 5)^2 with a dot starting at x = 0 and moving to the minimum point at x = 5 . Include labeled x and y axes. Use a black background.



================================================
FILE: datasets/code/ArgMinExample.py
================================================
from manim import *


class ArgMinExample(Scene):
    def construct(self):
        ax = Axes(
            x_range=[
                0, 10], y_range=[
                0, 100, 10], axis_config={
                "include_tip": False})
        labels = ax.get_axis_labels(x_label="x", y_label="f(x)")

        t = ValueTracker(0)

        def func(x):
            return 2 * (x - 5) ** 2
        graph = ax.plot(func, color=MAROON)

        initial_point = [
            ax.coords_to_point(
                t.get_value(), func(
                    t.get_value()))]
        dot = Dot(point=initial_point)

        dot.add_updater(
            lambda x: x.move_to(
                ax.c2p(
                    t.get_value(),
                    func(
                        t.get_value()))))
        x_space = np.linspace(*ax.x_range[:2], 200)
        minimum_index = func(x_space).argmin()

        self.add(ax, labels, graph, dot)
        self.play(t.animate.set_value(x_space[minimum_index]))
        self.wait()



================================================
FILE: datasets/code/BooleanOperations.py
================================================
from manim import *


class BooleanOperations(Scene):
    def construct(self):
        ellipse1 = Ellipse(
            width=4.0,
            height=5.0,
            fill_opacity=0.5,
            color=BLUE,
            stroke_width=10).move_to(LEFT)
        ellipse2 = ellipse1.copy().set_color(color=RED).move_to(RIGHT)
        bool_ops_text = MarkupText(
            "<u>Boolean Operation</u>").next_to(ellipse1, UP * 3)
        ellipse_group = Group(
            bool_ops_text,
            ellipse1,
            ellipse2).move_to(
            LEFT *
            3)
        self.play(FadeIn(ellipse_group))

        i = Intersection(ellipse1, ellipse2, color=GREEN, fill_opacity=0.5)
        self.play(i.animate.scale(0.25).move_to(RIGHT * 5 + UP * 2.5))
        intersection_text = Text("Intersection", font_size=23).next_to(i, UP)
        self.play(FadeIn(intersection_text))

        u = Union(ellipse1, ellipse2, color=ORANGE, fill_opacity=0.5)
        union_text = Text("Union", font_size=23)
        self.play(u.animate.scale(0.3).next_to(
            i, DOWN, buff=union_text.height * 3))
        union_text.next_to(u, UP)
        self.play(FadeIn(union_text))

        e = Exclusion(ellipse1, ellipse2, color=YELLOW, fill_opacity=0.5)
        exclusion_text = Text("Exclusion", font_size=23)
        self.play(
            e.animate.scale(0.3).next_to(
                u,
                DOWN,
                buff=exclusion_text.height *
                3.5))
        exclusion_text.next_to(e, UP)
        self.play(FadeIn(exclusion_text))

        d = Difference(ellipse1, ellipse2, color=PINK, fill_opacity=0.5)
        difference_text = Text("Difference", font_size=23)
        self.play(
            d.animate.scale(0.3).next_to(
                u,
                LEFT,
                buff=difference_text.height *
                3.5))
        difference_text.next_to(d, UP)
        self.play(FadeIn(difference_text))



================================================
FILE: datasets/code/BraceAnnotation.py
================================================
from manim import *


class BraceAnnotation(Scene):
    def construct(self):
        dot = Dot([-2, -1, 0])
        dot2 = Dot([2, 1, 0])
        line = Line(dot.get_center(), dot2.get_center()).set_color(ORANGE)
        b1 = Brace(line)
        b1text = b1.get_text("Horizontal distance")
        b2 = Brace(
            line, direction=line.copy().rotate(
                PI / 2).get_unit_vector())
        b2text = b2.get_tex("x-x_1")
        self.add(line, dot, dot2, b1, b2, b1text, b2text)



================================================
FILE: datasets/code/FixedInFrameMObjectTest.py
================================================
from manim import *


class FixedInFrameMObjectTest(ThreeDScene):
    def construct(self):
        axes = ThreeDAxes()
        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)
        text3d = Text("This is a 3D text")
        self.add_fixed_in_frame_mobjects(text3d)
        text3d.to_corner(UL)
        self.add(axes)
        self.wait()



================================================
FILE: datasets/code/FollowingGraphCamera.py
================================================
from manim import *


class FollowingGraphCamera(MovingCameraScene):
    def construct(self):
        self.camera.frame.save_state()

        # create the axes and the curve
        ax = Axes(x_range=[-1, 10], y_range=[-1, 10])
        graph = ax.plot(lambda x: np.sin(x), color=BLUE, x_range=[0, 3 * PI])

        # create dots based on the graph
        moving_dot = Dot(ax.i2gp(graph.t_min, graph), color=ORANGE)
        dot_1 = Dot(ax.i2gp(graph.t_min, graph))
        dot_2 = Dot(ax.i2gp(graph.t_max, graph))

        self.add(ax, graph, dot_1, dot_2, moving_dot)
        self.play(self.camera.frame.animate.scale(0.5).move_to(moving_dot))

        def update_curve(mob):
            mob.move_to(moving_dot.get_center())

        self.camera.frame.add_updater(update_curve)
        self.play(MoveAlongPath(moving_dot, graph, rate_func=linear))
        self.camera.frame.remove_updater(update_curve)

        self.play(Restore(self.camera.frame))



================================================
FILE: datasets/code/GradientImageFromArray.py
================================================
from manim import *


class GradientImageFromArray(Scene):
    def construct(self):
        n = 256
        imageArray = np.uint8(
            [[i * 256 / n for i in range(0, n)] for _ in range(0, n)]
        )
        image = ImageMobject(imageArray).scale(2)
        image.background_rectangle = SurroundingRectangle(image, GREEN)
        self.add(image, image.background_rectangle)



================================================
FILE: datasets/code/GraphAreaPlot.py
================================================
from manim import *


class GraphAreaPlot(Scene):
    def construct(self):
        ax = Axes(
            x_range=[0, 5],
            y_range=[0, 6],
            x_axis_config={"numbers_to_include": [2, 3]},
            tips=False,
        )

        labels = ax.get_axis_labels()

        curve_1 = ax.plot(
            lambda x: 4 *
            x -
            x ** 2,
            x_range=[
                0,
                4],
            color=BLUE_C)
        curve_2 = ax.plot(
            lambda x: 0.8 * x ** 2 - 3 * x + 4,
            x_range=[0, 4],
            color=GREEN_B,
        )

        line_1 = ax.get_vertical_line(
            ax.input_to_graph_point(
                2, curve_1), color=YELLOW)
        line_2 = ax.get_vertical_line(ax.i2gp(3, curve_1), color=YELLOW)

        riemann_area = ax.get_riemann_rectangles(
            curve_1,
            x_range=[
                0.3,
                0.6],
            dx=0.03,
            color=BLUE,
            fill_opacity=0.5)
        area = ax.get_area(
            curve_2, [
                2, 3], bounded_graph=curve_1, color=GREY, opacity=0.5)

        self.add(
            ax,
            labels,
            curve_1,
            curve_2,
            line_1,
            line_2,
            riemann_area,
            area)



================================================
FILE: datasets/code/HeatDiagramPlot.py
================================================
from manim import *


class HeatDiagramPlot(Scene):
    def construct(self):
        ax = Axes(
            x_range=[0, 40, 5],
            y_range=[-8, 32, 5],
            x_length=9,
            y_length=6,
            x_axis_config={"numbers_to_include": np.arange(0, 40, 5)},
            y_axis_config={"numbers_to_include": np.arange(-5, 34, 5)},
            tips=False,
        )
        labels = ax.get_axis_labels(
            x_label=Tex("$\\Delta Q$"), y_label=Tex("T[$^\\circ C$]")
        )

        x_vals = [0, 8, 38, 39]
        y_vals = [20, 0, 0, -5]
        graph = ax.plot_line_graph(x_values=x_vals, y_values=y_vals)

        self.add(ax, labels, graph)



================================================
FILE: datasets/code/ManimCELogo.py
================================================
from manim import *


class ManimCELogo(Scene):
    def construct(self):
        self.camera.background_color = "#ece6e2"
        logo_green = "#87c2a5"
        logo_blue = "#525893"
        logo_red = "#e07a5f"
        logo_black = "#343434"
        ds_m = MathTex(r"\mathbb{M}", fill_color=logo_black).scale(7)
        ds_m.shift(2.25 * LEFT + 1.5 * UP)
        circle = Circle(color=logo_green, fill_opacity=1).shift(LEFT)
        square = Square(color=logo_blue, fill_opacity=1).shift(UP)
        triangle = Triangle(color=logo_red, fill_opacity=1).shift(RIGHT)
        logo = VGroup(triangle, square, circle, ds_m)  # order matters
        logo.move_to(ORIGIN)
        self.add(logo)



================================================
FILE: datasets/code/MovingAngle.py
================================================
from manim import *


class MovingAngle(Scene):
    def construct(self):
        rotation_center = LEFT

        theta_tracker = ValueTracker(110)
        line1 = Line(LEFT, RIGHT)
        line_moving = Line(LEFT, RIGHT)
        line_ref = line_moving.copy()
        line_moving.rotate(
            theta_tracker.get_value() * DEGREES, about_point=rotation_center
        )
        a = Angle(line1, line_moving, radius=0.5, other_angle=False)
        tex = MathTex(r"\theta").move_to(
            Angle(
                line1,
                line_moving,
                radius=0.5 +
                3 *
                SMALL_BUFF,
                other_angle=False).point_from_proportion(0.5))

        self.add(line1, line_moving, a, tex)
        self.wait()

        line_moving.add_updater(
            lambda x: x.become(
                line_ref.copy()).rotate(
                theta_tracker.get_value() *
                DEGREES,
                about_point=rotation_center))

        a.add_updater(
            lambda x: x.become(
                Angle(
                    line1,
                    line_moving,
                    radius=0.5,
                    other_angle=False)))
        tex.add_updater(
            lambda x: x.move_to(
                Angle(
                    line1,
                    line_moving,
                    radius=0.5 +
                    3 *
                    SMALL_BUFF,
                    other_angle=False).point_from_proportion(0.5)))

        self.play(theta_tracker.animate.set_value(40))
        self.play(theta_tracker.animate.increment_value(140))
        self.play(tex.animate.set_color(RED), run_time=0.5)
        self.play(theta_tracker.animate.set_value(350))



================================================
FILE: datasets/code/MovingAround.py
================================================
from manim import *


class MovingAround(Scene):
    def construct(self):
        square = Square(color=BLUE, fill_opacity=1)

        self.play(square.animate.shift(LEFT))
        self.play(square.animate.set_fill(ORANGE))
        self.play(square.animate.scale(0.3))
        self.play(square.animate.rotate(0.4))



================================================
FILE: datasets/code/MovingDots.py
================================================
from manim import *


class MovingDots(Scene):
    def construct(self):
        d1, d2 = Dot(color=BLUE), Dot(color=GREEN)
        dg = VGroup(d1, d2).arrange(RIGHT, buff=1)
        l1 = Line(d1.get_center(), d2.get_center()).set_color(RED)
        x = ValueTracker(0)
        y = ValueTracker(0)
        d1.add_updater(lambda z: z.set_x(x.get_value()))
        d2.add_updater(lambda z: z.set_y(y.get_value()))
        l1.add_updater(
            lambda z: z.become(
                Line(
                    d1.get_center(),
                    d2.get_center())))
        self.add(d1, d2, l1)
        self.play(x.animate.set_value(5))
        self.play(y.animate.set_value(4))
        self.wait()



================================================
FILE: datasets/code/MovingFrameBox.py
================================================
from manim import *


class MovingFrameBox(Scene):
    def construct(self):
        text = MathTex(
            "\\frac{d}{dx}f(x)g(x)=", "f(x)\\frac{d}{dx}g(x)", "+",
            "g(x)\\frac{d}{dx}f(x)"
        )
        self.play(Write(text))
        framebox1 = SurroundingRectangle(text[1], buff=.1)
        framebox2 = SurroundingRectangle(text[3], buff=.1)
        self.play(
            Create(framebox1),
        )
        self.wait()
        self.play(
            ReplacementTransform(framebox1, framebox2),
        )
        self.wait()



================================================
FILE: datasets/code/MovingGroupToDestination.py
================================================
from manim import *


class MovingGroupToDestination(Scene):
    def construct(self):
        group = VGroup(
            Dot(LEFT), Dot(ORIGIN), Dot(
                RIGHT, color=RED), Dot(
                2 * RIGHT)).scale(1.4)
        dest = Dot([4, 3, 0], color=YELLOW)
        self.add(group, dest)
        self.play(
            group.animate.shift(
                dest.get_center() -
                group[2].get_center()))
        self.wait(0.5)



================================================
FILE: datasets/code/MovingZoomedSceneAround.py
================================================
from manim import *


class MovingZoomedSceneAround(ZoomedScene):
    # contributed by TheoremofBeethoven, www.youtube.com/c/TheoremofBeethoven
    def __init__(self, **kwargs):
        ZoomedScene.__init__(
            self,
            zoom_factor=0.3,
            zoomed_display_height=1,
            zoomed_display_width=6,
            image_frame_stroke_width=20,
            zoomed_camera_config={
                "default_frame_stroke_width": 3,
            },
            **kwargs
        )

    def construct(self):
        dot = Dot().shift(UL * 2)
        image = ImageMobject(np.uint8([[0, 100, 30, 200],
                                       [255, 0, 5, 33]]))
        image.height = 7
        frame_text = Text("Frame", color=PURPLE, font_size=67)
        zoomed_camera_text = Text("Zoomed camera", color=RED, font_size=67)

        self.add(image, dot)
        zoomed_camera = self.zoomed_camera
        zoomed_display = self.zoomed_display
        frame = zoomed_camera.frame
        zoomed_display_frame = zoomed_display.display_frame

        frame.move_to(dot)
        frame.set_color(PURPLE)
        zoomed_display_frame.set_color(RED)
        zoomed_display.shift(DOWN)

        zd_rect = BackgroundRectangle(
            zoomed_display,
            fill_opacity=0,
            buff=MED_SMALL_BUFF)
        self.add_foreground_mobject(zd_rect)

        unfold_camera = UpdateFromFunc(
            zd_rect, lambda rect: rect.replace(zoomed_display))

        frame_text.next_to(frame, DOWN)

        self.play(Create(frame), FadeIn(frame_text, shift=UP))
        self.activate_zooming()

        self.play(self.get_zoomed_display_pop_out_animation(), unfold_camera)
        zoomed_camera_text.next_to(zoomed_display_frame, DOWN)
        self.play(FadeIn(zoomed_camera_text, shift=UP))
        # Scale in        x   y  z
        scale_factor = [0.5, 1.5, 0]
        self.play(
            frame.animate.scale(scale_factor),
            zoomed_display.animate.scale(scale_factor),
            FadeOut(zoomed_camera_text),
            FadeOut(frame_text)
        )
        self.wait()
        self.play(ScaleInPlace(zoomed_display, 2))
        self.wait()
        self.play(frame.animate.shift(2.5 * DOWN))
        self.wait()
        self.play(self.get_zoomed_display_pop_out_animation(),
                  unfold_camera, rate_func=lambda t: smooth(1 - t))
        self.play(Uncreate(zoomed_display_frame), FadeOut(frame))
        self.wait()



================================================
FILE: datasets/code/OpeningManim.py
================================================
from manim import *


class OpeningManim(Scene):
    def construct(self):
        title = Tex(r"This is some \LaTeX")
        basel = MathTex(r"\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}")
        VGroup(title, basel).arrange(DOWN)
        self.play(
            Write(title),
            FadeIn(basel, shift=DOWN),
        )
        self.wait()

        transform_title = Tex("That was a transform")
        transform_title.to_corner(UP + LEFT)
        self.play(
            Transform(title, transform_title),
            LaggedStart(*[FadeOut(obj, shift=DOWN) for obj in basel]),
        )
        self.wait()

        grid = NumberPlane()
        grid_title = Tex("This is a grid", font_size=72)
        grid_title.move_to(transform_title)

        self.add(grid, grid_title)  # Make sure title is on top of grid
        self.play(
            FadeOut(title),
            FadeIn(grid_title, shift=UP),
            Create(grid, run_time=3, lag_ratio=0.1),
        )
        self.wait()

        grid_transform_title = Tex(
            r"That was a non-linear function \\ applied to the grid"
        )
        grid_transform_title.move_to(grid_title, UL)
        grid.prepare_for_nonlinear_transform()
        self.play(
            grid.animate.apply_function(
                lambda p: p
                + np.array(
                    [
                        np.sin(p[1]),
                        np.sin(p[0]),
                        0,
                    ]
                )
            ),
            run_time=3,
        )
        self.wait()
        self.play(Transform(grid_title, grid_transform_title))
        self.wait()



================================================
FILE: datasets/code/PointMovingOnShapes.py
================================================
from manim import *


class PointMovingOnShapes(Scene):
    def construct(self):
        circle = Circle(radius=1, color=BLUE)
        dot = Dot()
        dot2 = dot.copy().shift(RIGHT)
        self.add(dot)

        line = Line([3, 0, 0], [5, 0, 0])
        self.add(line)

        self.play(GrowFromCenter(circle))
        self.play(Transform(dot, dot2))
        self.play(MoveAlongPath(dot, circle), run_time=2, rate_func=linear)
        self.play(Rotating(dot, about_point=[2, 0, 0]), run_time=1.5)
        self.wait()



================================================
FILE: datasets/code/PointWithTrace.py
================================================
from manim import *


class PointWithTrace(Scene):
    def construct(self):
        path = VMobject()
        dot = Dot()
        path.set_points_as_corners([dot.get_center(), dot.get_center()])

        def update_path(path):
            previous_path = path.copy()
            previous_path.add_points_as_corners([dot.get_center()])
            path.become(previous_path)
        path.add_updater(update_path)
        self.add(path, dot)
        self.play(Rotating(dot, radians=PI, about_point=RIGHT, run_time=2))
        self.wait()
        self.play(dot.animate.shift(UP))
        self.play(dot.animate.shift(LEFT))
        self.wait()



================================================
FILE: datasets/code/PolygonOnAxes.py
================================================
from manim import *


class PolygonOnAxes(Scene):
    def get_rectangle_corners(self, bottom_left, top_right):
        return [
            (top_right[0], top_right[1]),
            (bottom_left[0], top_right[1]),
            (bottom_left[0], bottom_left[1]),
            (top_right[0], bottom_left[1]),
        ]

    def construct(self):
        ax = Axes(
            x_range=[0, 10],
            y_range=[0, 10],
            x_length=6,
            y_length=6,
            axis_config={"include_tip": False},
        )

        t = ValueTracker(5)
        k = 25

        graph = ax.plot(
            lambda x: k / x,
            color=YELLOW_D,
            x_range=[k / 10, 10.0, 0.01],
            use_smoothing=False,
        )

        def get_rectangle():
            polygon = Polygon(
                *[
                    ax.c2p(*i)
                    for i in self.get_rectangle_corners(
                        (0, 0), (t.get_value(), k / t.get_value())
                    )
                ]
            )
            polygon.stroke_width = 1
            polygon.set_fill(BLUE, opacity=0.5)
            polygon.set_stroke(YELLOW_B)
            return polygon

        polygon = always_redraw(get_rectangle)

        dot = Dot()
        dot.add_updater(
            lambda x: x.move_to(
                ax.c2p(
                    t.get_value(),
                    k / t.get_value())))
        dot.set_z_index(10)

        self.add(ax, graph, dot)
        self.play(Create(polygon))
        self.play(t.animate.set_value(10))
        self.play(t.animate.set_value(k / 10))
        self.play(t.animate.set_value(5))



================================================
FILE: datasets/code/RotationUpdater.py
================================================
from manim import *


class RotationUpdater(Scene):
    def construct(self):
        def updater_forth(mobj, dt):
            mobj.rotate_about_origin(dt)

        def updater_back(mobj, dt):
            mobj.rotate_about_origin(-dt)
        line_reference = Line(ORIGIN, LEFT).set_color(WHITE)
        line_moving = Line(ORIGIN, LEFT).set_color(YELLOW)
        line_moving.add_updater(updater_forth)
        self.add(line_reference, line_moving)
        self.wait(2)
        line_moving.remove_updater(updater_forth)
        line_moving.add_updater(updater_back)
        self.wait(2)
        line_moving.remove_updater(updater_back)
        self.wait(0.5)



================================================
FILE: datasets/code/SinAndCosFunctionPlot.py
================================================
from manim import *


class SinAndCosFunctionPlot(Scene):
    def construct(self):
        axes = Axes(
            x_range=[-10, 10.3, 1],
            y_range=[-1.5, 1.5, 1],
            x_length=10,
            axis_config={"color": GREEN},
            x_axis_config={
                "numbers_to_include": np.arange(-10, 10.01, 2),
                "numbers_with_elongated_ticks": np.arange(-10, 10.01, 2),
            },
            tips=False,
        )
        axes_labels = axes.get_axis_labels()
        sin_graph = axes.plot(lambda x: np.sin(x), color=BLUE)
        cos_graph = axes.plot(lambda x: np.cos(x), color=RED)

        sin_label = axes.get_graph_label(
            sin_graph, "\\sin(x)", x_val=-10, direction=UP / 2
        )
        cos_label = axes.get_graph_label(cos_graph, label="\\cos(x)")

        vert_line = axes.get_vertical_line(
            axes.i2gp(TAU, cos_graph), color=YELLOW, line_func=Line
        )
        line_label = axes.get_graph_label(
            cos_graph, "x=2\\pi", x_val=TAU, direction=UR, color=WHITE
        )

        plot = VGroup(axes, sin_graph, cos_graph, vert_line)
        labels = VGroup(axes_labels, sin_label, cos_label, line_label)
        self.add(plot, labels)



================================================
FILE: datasets/code/SineCurveUnitCircle.py
================================================
from manim import *


class SineCurveUnitCircle(Scene):
    # contributed by heejin_park, https://infograph.tistory.com/230
    def construct(self):
        self.show_axis()
        self.show_circle()
        self.move_dot_and_draw_curve()
        self.wait()

    def show_axis(self):
        x_start = np.array([-6, 0, 0])
        x_end = np.array([6, 0, 0])

        y_start = np.array([-4, -2, 0])
        y_end = np.array([-4, 2, 0])

        x_axis = Line(x_start, x_end)
        y_axis = Line(y_start, y_end)

        self.add(x_axis, y_axis)
        self.add_x_labels()

        self.origin_point = np.array([-4, 0, 0])
        self.curve_start = np.array([-3, 0, 0])

    def add_x_labels(self):
        x_labels = [
            MathTex("\\pi"), MathTex("2 \\pi"),
            MathTex("3 \\pi"), MathTex("4 \\pi"),
        ]

        for i in range(len(x_labels)):
            x_labels[i].next_to(np.array([-1 + 2 * i, 0, 0]), DOWN)
            self.add(x_labels[i])

    def show_circle(self):
        circle = Circle(radius=1)
        circle.move_to(self.origin_point)
        self.add(circle)
        self.circle = circle

    def move_dot_and_draw_curve(self):
        orbit = self.circle
        origin_point = self.origin_point

        dot = Dot(radius=0.08, color=YELLOW)
        dot.move_to(orbit.point_from_proportion(0))
        self.t_offset = 0
        rate = 0.25

        def go_around_circle(mob, dt):
            self.t_offset += (dt * rate)
            # print(self.t_offset)
            mob.move_to(orbit.point_from_proportion(self.t_offset % 1))

        def get_line_to_circle():
            return Line(origin_point, dot.get_center(), color=BLUE)

        def get_line_to_curve():
            x = self.curve_start[0] + self.t_offset * 4
            y = dot.get_center()[1]
            return Line(dot.get_center(), np.array(
                [x, y, 0]), color=YELLOW_A, stroke_width=2)

        self.curve = VGroup()
        self.curve.add(Line(self.curve_start, self.curve_start))

        def get_curve():
            last_line = self.curve[-1]
            x = self.curve_start[0] + self.t_offset * 4
            y = dot.get_center()[1]
            new_line = Line(last_line.get_end(), np.array(
                [x, y, 0]), color=YELLOW_D)
            self.curve.add(new_line)

            return self.curve

        dot.add_updater(go_around_circle)

        origin_to_circle_line = always_redraw(get_line_to_circle)
        dot_to_curve_line = always_redraw(get_line_to_curve)
        sine_curve_line = always_redraw(get_curve)

        self.add(dot)
        self.add(
            orbit,
            origin_to_circle_line,
            dot_to_curve_line,
            sine_curve_line)
        self.wait(8.5)

        dot.remove_updater(go_around_circle)



================================================
FILE: datasets/code/ThreeDCameraIllusionRotation.py
================================================
from manim import *


class ThreeDCameraIllusionRotation(ThreeDScene):
    def construct(self):
        axes = ThreeDAxes()
        circle = Circle()
        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)
        self.add(circle, axes)
        self.begin_3dillusion_camera_rotation(rate=2)
        self.wait(PI / 2)
        self.stop_3dillusion_camera_rotation()



================================================
FILE: datasets/code/ThreeDCameraRotation.py
================================================
from manim import *


class ThreeDCameraRotation(ThreeDScene):
    def construct(self):
        axes = ThreeDAxes()
        circle = Circle()
        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)
        self.add(circle, axes)
        self.begin_ambient_camera_rotation(rate=0.1)
        self.wait()
        self.stop_ambient_camera_rotation()
        self.move_camera(phi=75 * DEGREES, theta=30 * DEGREES)
        self.wait()



================================================
FILE: datasets/code/ThreeDLightSourcePosition.py
================================================
from manim import *


class ThreeDLightSourcePosition(ThreeDScene):
    def construct(self):
        axes = ThreeDAxes()
        sphere = Surface(
            lambda u, v: np.array([
                1.5 * np.cos(u) * np.cos(v),
                1.5 * np.cos(u) * np.sin(v),
                1.5 * np.sin(u)
            ]), v_range=[0, TAU], u_range=[-PI / 2, PI / 2],
            checkerboard_colors=[RED_D, RED_E], resolution=(15, 32)
        )
        self.renderer.camera.light_source.move_to(
            3 * IN)  # changes the source of the light
        self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)
        self.add(axes, sphere)



================================================
FILE: datasets/code/ThreeDSurfacePlot.py
================================================
from manim import *


class ThreeDSurfacePlot(ThreeDScene):
    def construct(self):
        resolution_fa = 24
        self.set_camera_orientation(phi=75 * DEGREES, theta=-30 * DEGREES)

        def param_gauss(u, v):
            x = u
            y = v
            sigma, mu = 0.4, [0.0, 0.0]
            d = np.linalg.norm(np.array([x - mu[0], y - mu[1]]))
            z = np.exp(-(d ** 2 / (2.0 * sigma ** 2)))
            return np.array([x, y, z])

        gauss_plane = Surface(
            param_gauss,
            resolution=(resolution_fa, resolution_fa),
            v_range=[-2, +2],
            u_range=[-2, +2]
        )

        gauss_plane.scale(2, about_point=ORIGIN)
        gauss_plane.set_style(fill_opacity=1, stroke_color=GREEN)
        gauss_plane.set_fill_by_checkerboard(ORANGE, BLUE, opacity=0.5)
        axes = ThreeDAxes()
        self.add(axes, gauss_plane)



================================================
FILE: datasets/code/VectorArrow.py
================================================
from manim import *


class VectorArrow(Scene):
    def construct(self):
        dot = Dot(ORIGIN)
        arrow = Arrow(ORIGIN, [2, 2, 0], buff=0)
        numberplane = NumberPlane()
        origin_text = Text('(0, 0)').next_to(dot, DOWN)
        tip_text = Text('(2, 2)').next_to(arrow.get_end(), RIGHT)
        self.add(numberplane, dot, arrow, origin_text, tip_text)



================================================
FILE: datasets/extract/edoh_dataset.py
================================================
import json
from datetime import datetime
from datasets import load_dataset

# Load the dataset
dataset = load_dataset("Edoh/manim_python")

# Prepare the output file name with a timestamp
timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
output_file = f"edoh-dataset-{timestamp}.jsonl"

# Open the file for writing
with open(output_file, 'w') as f:
    # Iterate over each example in the dataset
    for example in dataset['train']:
        # Create the structured message
        entry = {
            "messages": [
                {"role": "system", "content": "Write Manim scripts for animations in Python. Generate code, not text."},
                {"role": "user", "content": example['instruction']},
                {"role": "assistant", "content": example['output']}
            ]
        }
        # Write the JSON entry to the file
        f.write(json.dumps(entry) + '\n')

print(f"Dataset converted and saved to {output_file}")



================================================
FILE: datasets/format/formatter_edoh.py
================================================
"""
This file houses the various formatting methods of the edoh
note that I can only guarantee that it will work properly on the josnl file of the edoh
---
e.g input:
example_str = "from manim import * class MyScene(Scene): def construct(self): heptagon = RegularPolygon(n=7, radius=3, color=GREEN) self.add(heptagon)"
print(format_edoh(example_str))

e.g output:
>>>from manim import *
class MyScene(Scene):
        def construct(self):
                heptagon = RegularPolygon(n=7, radius=3, color=GREEN)
                self.add(heptagon)

"""

import json

def format_edoh(input_str: str)->str:
    '''
    
    '''
    str_import = input_str[:19]  # "from manim import *"
    str_class = input_str[20:41]  # "class MyScene(Scene):"
    str_def = input_str[42:62]  # "def construct(self):"
    str_content = input_str[63:]  # rest of code

    space_index = [i for i, c in enumerate(str_content) if c == " "]  # find all [space] index
    filtering_index = []
    for index in space_index:
        left_str = str_content[index - 1]
        right_str = str_content[index + 1]

        # If "=" is found on the left or right, then this is an assignment statement
        if left_str == "=" or right_str == "=":continue

        # If "," found on the left or right, then this is some function argument
        if left_str == "," or right_str == ",":continue

        # If "+-*/" found on the left or right, then this is an expression
        if left_str == "*" or right_str == "*":continue
        if left_str == "+" or right_str == "+":continue
        if left_str == "-" or right_str == "-":continue
        if left_str == "/" or right_str == "/":continue
        if left_str == "//" or right_str == "//":continue
        if left_str == "**" or right_str == "**":continue
        if left_str == "%" or right_str == "%":continue

        filtering_index.append(index+1)

    # Add beginning and ending indexes, e.g[0,...,...,...,76]
    filtering_index =[0] + filtering_index + [len(str_content)] 
    filtering_context = ''
    last_index = 0
    # Slice every two indexes and then add formatting symbols
    for index in filtering_index:
        if index == 0 or last_index == len(str_content):
            last_index = index
            continue 
        else:
            #print(last_index,index)
            if filtering_context =='':
                filtering_context = '\t\t'+str_content[last_index:index]
            else:
                filtering_context = filtering_context + '\n\t\t'+str_content[last_index:index]
            last_index = index
       
    # print(str_content, space_index,filtering_index,filtering_context)

    return (str_import+'\n'
            +str_class+'\n'
            +'\t'+str_def+'\n'
            +filtering_context)


if __name__ == "__main__":
    example = "from manim import * class MyScene(Scene): def construct(self): heptagon = RegularPolygon(n=7, radius=3, color=GREEN) self.add(heptagon)"

    with open(r'D:\GitHub\generative-manim\datasets\edoh-dataset.jsonl') as file:
        file = file.readlines()
        for data_line in file:
            decode = json.loads(data_line)
            print(decode['messages'][0])
            decode['messages'][2]['content'] = format_edoh(decode['messages'][2]['content'])

            with open(r'D:\GitHub\generative-manim\datasets\edoh-dataset-format.jsonl','a') as outfile:
                outfile.writelines(json.dumps(decode)+'\n')


    



================================================
FILE: datasets/prompt/API_Key.cfg
================================================
[qianfan]
QIANFAN_AK=PUT_YUOUR_API_KEY_HERE
QIANFAN_SK=PUT_YUOUR_Secret_KEY_HERE


================================================
FILE: datasets/prompt/prompt_from_OpenAIGPT.py
================================================
"""
This file is designed to invoke OpenAI GPT-4
to generate possible prompt words for specifying manim code.
"""

import os
import openai
from openai import OpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Define the GPT-4 model endpoint
llm = ChatOpenAI(model="gpt-4", openai_api_key=os.environ["OPENAI_API_KEY"])

# input and output
chain = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "If you are a user who is going to use GPT to generate manim code, write the prompt words used by the role according to the code I gave you, write in the same paragraph, do not need to step by step analysis, do not explain your output in the first paragraph:",
        ),
        (
            "user",
            "{input}",
        ),
    ]
)

reply = chain.format_messages(
    input="from manim import * class MyScene(Scene): def construct(self): circle = Circle(radius=2, color=BLUE) self.add(circle)"
)

print(llm(reply).content)
"""
e.g output
Generate the Manim code for me that creates a scene with a blue circle of radius 2 centered at the origin. The circle should appear gradually on the screen from left to right. Include comments in the code to explain each step.
"""



================================================
FILE: datasets/prompt/prompt_from_QianFan.py
================================================
"""
This file is designed to invoke Wenxinyi GPT under Baidu QianFan integration 
to generate possible prompt words for specifying manim code.
---
To use it, add the corresponding Key in API_key.cfg'''QIANFAN_AK/QIANFAN_SK'''

External file:./API_Key.cfg
Depends:pip install qianfan>=0.3.11
"""

import os
import configparser
from langchain_community.chat_models import QianfanChatEndpoint
from langchain_core.language_models.chat_models import HumanMessage
from langchain_core.prompts import ChatPromptTemplate

ABS_PATH = os.path.dirname(os.path.abspath(__file__))

assert os.path.exists(
    f"{ABS_PATH}/API_Key.cfg"
), f"can not find the {ABS_PATH}/API_Key.cfg"

# Load Setttings
CFG_API_KEY = configparser.ConfigParser()
CFG_API_KEY.read(f"{ABS_PATH}/API_Key.cfg", encoding="utf-8")

os.environ["QIANFAN_AK"] = str(CFG_API_KEY["qianfan"]["QIANFAN_AK"])
os.environ["QIANFAN_SK"] = str(CFG_API_KEY["qianfan"]["QIANFAN_SK"])
print(os.environ["QIANFAN_AK"], os.environ["QIANFAN_SK"])

# Chat stream, integrated with LangChain
chat_QianFan = QianfanChatEndpoint(
    streaming=False,
    model="ERNIE-3.5-8K",  # Well, this one is free
)

# input and output
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "If you are a user who is going to use GPT to generate manim code, write the prompt words used by the role according to the code I gave you, write in the same paragraph, do not need to step by step analysis, do not explain your output in the first paragraph:",
        ),
        ("user", "{input}"),
    ]
)
chain = prompt | chat_QianFan
reply = chain.invoke(
    {
        "input": "from manim import * class MyScene(Scene): def construct(self): circle = Circle(radius=2, color=BLUE) self.add(circle)"
    }
)
print(reply.content)
"""
e.g output
Generate the Manim code for me that creates a scene with a blue circle of radius 2 centered at the origin. The circle should appear gradually on the screen from left to right. Include comments in the code to explain each step.
"""




================================================
FILE: experiments/README.md
================================================
# Generative Manim Experiments

Do you feel that a new model or a new workflow can make a difference? Feel free to make a [Pull Request](https://github.com/360macky/generative-manim/pulls) to add it to this experiments folder.

We will progressively add them to the demo as well.



================================================
FILE: experiments/generative_manim_claude_sonnet.py
================================================
# -*- coding: utf-8 -*-
"""Generative Manim Claude Sonnet

Support for code generation in Manim with Claude Sonnet
"""

import os
from anthropic import Anthropic

client = Anthropic(
    api_key=os.environ.get("ANTHROPIC_API_KEY"),
)

user_prompt = input("Input the prompt to generate code: ")

# TODO: Sytem prompt to be enhanced
CLAUDE_SYSTEM_INSTRUCTIONS = """Write Manim scripts for animations in Python. Generate code, not text. Never explain code. Never add functions. Never add comments. Never infinte loops. Never use other library than Manim/math. Only complete the code block. Use variables with length of maximum 2 characters. At the end use 'self.play'.

```
from manim import *
from math import *

class GenScene(Scene):
    def construct(self):
        # Write here
```"""

message = client.messages.create(
    max_tokens=1024,
    system=CLAUDE_SYSTEM_INSTRUCTIONS,
    messages=[
        {
            "role": "user",
            "content": user_prompt,
        }
    ],
    model="claude-3-sonnet-20240229",
)
print(message.content)



================================================
FILE: experiments/Generative_Manim_LangGraph_Implementation.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Generative Manim LangGraph Implementation

Taking the example of [Code generation with flow](https://github.com/langchain-ai/langgraph/blob/main/examples/code_assistant/langgraph_code_assistant.ipynb?ref=blog.langchain.dev), we will implement a similar approach to generate code for Manim animations. So far, I think we would not need test validation, we can delay this step for later.
"""

! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4
# Output:
#   Collecting langchain_community

#     Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.9/1.9 MB[0m [31m7.8 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting langchain-openai

#     Downloading langchain_openai-0.1.3-py3-none-any.whl (33 kB)

#   Collecting langchain-anthropic

#     Downloading langchain_anthropic-0.1.11-py3-none-any.whl (16 kB)

#   Collecting langchain

#     Downloading langchain-0.1.16-py3-none-any.whl (817 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m817.7/817.7 kB[0m [31m6.1 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting langgraph

#     Downloading langgraph-0.0.38-py3-none-any.whl (59 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m59.3/59.3 kB[0m [31m5.0 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting bs4

#     Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)

#   Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)

#   Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.29)

#   Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)

#   Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)

#     Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)

#   Collecting langchain-core<0.2.0,>=0.1.45 (from langchain_community)

#     Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m291.3/291.3 kB[0m [31m10.0 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)

#     Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m115.2/115.2 kB[0m [31m6.9 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)

#   Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)

#   Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.2.3)

#   Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)

#     Downloading openai-1.23.2-py3-none-any.whl (311 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m311.2/311.2 kB[0m [31m12.6 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting tiktoken<1,>=0.5.2 (from langchain-openai)

#     Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.8/1.8 MB[0m [31m18.5 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting anthropic<1,>=0.23.0 (from langchain-anthropic)

#     Downloading anthropic-0.25.6-py3-none-any.whl (870 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m870.7/870.7 kB[0m [31m21.5 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from langchain-anthropic) (0.7.1)

#   Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)

#   Collecting jsonpatch<2.0,>=1.33 (from langchain)

#     Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)

#   Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)

#     Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)

#   Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)

#   Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)

#   Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)

#   Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)

#   Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)

#   Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)

#   Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)

#   Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (3.7.1)

#   Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (1.7.0)

#   Collecting httpx<1,>=0.23.0 (from anthropic<1,>=0.23.0->langchain-anthropic)

#     Downloading httpx-0.27.0-py3-none-any.whl (75 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m75.6/75.6 kB[0m [31m5.0 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (1.3.1)

#   Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (0.15.2)

#   Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.23.0->langchain-anthropic) (4.11.0)

#   Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)

#     Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m49.4/49.4 kB[0m [31m4.3 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)

#     Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)

#   Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)

#     Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)

#   Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.45->langchain_community)

#     Downloading packaging-23.2-py3-none-any.whl (53 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m53.0/53.0 kB[0m [31m4.8 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)

#     Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m141.1/141.1 kB[0m [31m2.8 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)

#   Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)

#   Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)

#   Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)

#   Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)

#   Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)

#   Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)

#   Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)

#   Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)

#   Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)

#   Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.23.0->langchain-anthropic) (1.2.0)

#   Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic<1,>=0.23.0->langchain-anthropic)

#     Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m77.9/77.9 kB[0m [31m5.2 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.23.0->langchain-anthropic)

#     Downloading h11-0.14.0-py3-none-any.whl (58 kB)

#   [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.3/58.3 kB[0m [31m2.3 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (0.20.3)

#   Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)

#     Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)

#   Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (3.13.4)

#   Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.23.0->langchain-anthropic) (2023.6.0)

#   Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, tiktoken, marshmallow, jsonpatch, httpcore, bs4, langsmith, httpx, dataclasses-json, openai, langchain-core, anthropic, langgraph, langchain-text-splitters, langchain-openai, langchain_community, langchain-anthropic, langchain

#     Attempting uninstall: packaging

#       Found existing installation: packaging 24.0

#       Uninstalling packaging-24.0:

#         Successfully uninstalled packaging-24.0

#   Successfully installed anthropic-0.25.6 bs4-0.0.2 dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-anthropic-0.1.11 langchain-core-0.1.45 langchain-openai-0.1.3 langchain-text-splitters-0.0.1 langchain_community-0.0.34 langgraph-0.0.38 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.23.2 orjson-3.10.1 packaging-23.2 tiktoken-0.6.0 typing-inspect-0.9.0


!pip install anthropic
# Output:
#   Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (0.25.6)

#   Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)

#   Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)

#   Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.27.0)

#   Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.7.0)

#   Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)

#   Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.15.2)

#   Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.11.0)

#   Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)

#   Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.0)

#   Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)

#   Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.5)

#   Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)

#   Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.6.0)

#   Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.18.1)

#   Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.20.3)

#   Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.13.4)

#   Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)

#   Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)

#   Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.2)

#   Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)

#   Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)

#   Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)

#   Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)


"""
## Extracting examples from Manim docs
"""

from bs4 import BeautifulSoup as Soup
from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader

# Manim Examples docs
url = "https://docs.manim.community/en/stable/examples.html"
loader = RecursiveUrlLoader(
    url=url, max_depth=20, extractor=lambda x: Soup(x, "html.parser").text
)
docs = loader.load()

# Sort the list based on the URLs and get the text
d_sorted = sorted(docs, key=lambda x: x.metadata["source"])
d_reversed = list(reversed(d_sorted))
concatenated_content = "\n\n\n --- \n\n\n".join(
    [doc.page_content for doc in d_reversed]
)

"""
## LLM Solution
"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

### OpenAI

# Manim code generation prompt
code_gen_prompt = ChatPromptTemplate.from_messages(
    [("system","""You are a coding assistant with expertise in Manim, Graphical Animation Library. \n
    Here is a full set of Manim documentation:  \n ------- \n  {context} \n ------- \n Answer the user
    question based on the above provided documentation. Ensure any code you provide can be executed \n
    with all required imports and variables defined. Structure your answer with a description of the code solution. \n
    Then list the imports. And finally list the functioning code block. Here is the user question:"""),
    ("placeholder", "{messages}")]
)

class code(BaseModel):
    """Code output"""
    prefix: str = Field(description="Description of the problem and approach")
    imports: str = Field(description="Code block import statements")
    code: str = Field(description="Code block not including import statements")
    description = "Schema for code solutions to requests on code for Manim Animations."

from google.colab import userdata
import anthropic

OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')
anthropic.api_key = userdata.get('ANTHROPIC_API_KEY')

expt_llm = "gpt-4-0125-preview"
llm = ChatOpenAI(temperature=0, model=expt_llm, openai_api_key=OPENAI_API_KEY)
code_gen_chain = code_gen_prompt | llm.with_structured_output(code)

question = "Draw three red circles"

solution = code_gen_chain.invoke({"context":concatenated_content,"messages":[("user",question)]})

solution
# Output:
#   code(prefix='Draw three red circles using Manim', imports='from manim import *', code='class ThreeRedCircles(Scene):\n    def construct(self):\n        # Create three red circles\n        circle1 = Circle(color=RED).shift(LEFT)\n        circle2 = Circle(color=RED)\n        circle3 = Circle(color=RED).shift(RIGHT)\n\n        # Add circles to the scene\n        self.add(circle1, circle2, circle3)\n\n# To render the scene, uncomment the following line and run the script\n# scene = ThreeRedCircles()\n# scene.render()', description='Schema for code solutions to requests on code for Manim Animations.')

"""
Now let's try with Anthropic
"""

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

code_gen_prompt_claude = ChatPromptTemplate.from_messages(
    [("system","""<instructions> You are a coding assistant with expertise in Manim, Graphical Animation Library. \n
    Here is a full set of Manim documentation:  \n ------- \n  {context} \n ------- \n Answer the user request based on the \n
    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \n
    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \n
    Invoke the code tool from Manim to structure the output correctly. </instructions> \n Here is the user request:""",),
    ("placeholder", "{messages}"),])

# Data model
class code(BaseModel):
    """Code output"""

    prefix: str = Field(description="Description of the problem and approach")
    imports: str = Field(description="Code block import statements")
    code: str = Field(description="Code block not including import statements")
    description = "Schema for code solutions to questions about Manim."

# LLM
# expt_llm = "claude-3-haiku-20240307"
expt_llm = "claude-3-opus-20240229"
llm = ChatAnthropic(
    model=expt_llm,
    default_headers={"anthropic-beta": "tools-2024-04-04"},
)

structured_llm_claude = llm.with_structured_output(code, include_raw=True)
# Output:
#   /usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.

#     warn_beta(


# Optional: Check for errors in case tool use is flaky
def check_claude_output(tool_output):
    """Check for parse error or failure to call the tool"""

    # Error with parsing
    if tool_output["parsing_error"]:
        # Report back output and parsing errors
        print("Parsing error!")
        raw_output = str(code_output["raw"].content)
        error = tool_output["parsing_error"]
        raise ValueError(
            f"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \n Parse error: {error}"
        )

    # Tool was not invoked
    elif not tool_output["parsed"]:
        print("Failed to invoke tool!")
        raise ValueError(
            f"You did not use the provided tool! Be sure to invoke the tool to structure the output."
        )
    return tool_output

code_chain_claude_raw = code_gen_prompt_claude | structured_llm_claude | check_claude_output

def insert_errors(inputs):
    """Insert errors for tool parsing in the messages"""

    # Get errors
    error = inputs["error"]
    messages = inputs["messages"]
    messages += [
        (
            "assistant",
            f"Retry. You are required to fix the parsing errors: {error} \n\n You must invoke the provided tool.",
        )
    ]
    return {
        "messages": messages,
        "context": inputs["context"],
    }


# This will be run as a fallback chain
fallback_chain = insert_errors | code_chain_claude_raw
N = 3  # Max re-tries
code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(fallbacks=[fallback_chain] * N, exception_key="error")

def parse_output(solution):
    """When we add 'include_raw=True' to structured output,
       it will return a dict w 'raw', 'parsed', 'parsing_error'. """

    return solution['parsed']

# Wtih re-try to correct for failure to invoke tool
# TODO: Annoying errors w/ "user" vs "assistant"
# Roles must alternate between "user" and "assistant", but found multiple "user" roles in a row
code_gen_chain = code_gen_chain_re_try | parse_output

# No re-try
code_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output

# Test
question = "Draw a red circle"
solution = code_gen_chain.invoke({"context":concatenated_content,"messages":[("user",question)]})
solution
# Output:
#   code(prefix='Draw a red circle using Manim', imports='from manim import *', code='class RedCircle(Scene):\n    def construct(self):\n        circle = Circle(radius=1, color=RED)\n        self.add(circle)\n        self.wait(2)\n\n# To run this scene, use the following command in your terminal:\n# manim -p -ql <script_name>.py RedCircle', description='Schema for code solutions to questions about Manim.')

"""
## State

Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation.
"""

from typing import Dict, TypedDict, List

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        error : Binary flag for control flow to indicate whether test error was tripped
        messages : With user question, error messages, reasoning
        generation : Code solution
        iterations : Number of tries
    """

    error : str
    messages : List
    generation : str
    iterations : int

"""
## Graph

Our graph lays out the logical flow shown in the figure above.


"""

from operator import itemgetter
from langchain.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import RunnablePassthrough

### Parameter

# Max tries
max_iterations = 3
# Reflect
# flag = 'reflect'
flag = 'do not reflect'

### Nodes

def generate(state: GraphState):
    """
    Generate a code solution

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation
    """

    print("---GENERATING CODE SOLUTION---")

    # State
    messages = state["messages"]
    iterations = state["iterations"]
    error = state["error"]

    # We have been routed back to generation with an error
    if error == "yes":
        messages += [("user","Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:")]

    # Solution
    code_solution = code_gen_chain.invoke({"context": concatenated_content, "messages" : messages})
    messages += [("assistant",f"{code_solution.prefix} \n Imports: {code_solution.imports} \n Code: {code_solution.code}")]

    # Increment
    iterations = iterations + 1
    return {"generation": code_solution, "messages": messages, "iterations": iterations}

def code_check(state: GraphState):
    """
    Check code

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, error
    """

    print("---CHECKING CODE---")

    # State
    messages = state["messages"]
    code_solution = state["generation"]
    iterations = state["iterations"]

    # Get solution components
    prefix = code_solution.prefix
    imports = code_solution.imports
    code = code_solution.code

    # Check imports
    try:
        exec(imports)
    except Exception as e:
        print("---CODE IMPORT CHECK: FAILED---")
        error_message = [("user", f"Your solution failed the import test: {e}")]
        messages += error_message
        return {"generation": code_solution, "messages": messages, "iterations": iterations, "error": "yes"}

    # Check execution
    try:
        exec(imports + "\n" + code)
    except Exception as e:
        print("---CODE BLOCK CHECK: FAILED---")
        error_message = [("user", f"Your solution failed the code execution test: {e}")]
        messages += error_message
        return {"generation": code_solution, "messages": messages, "iterations": iterations, "error": "yes"}

    # No errors
    print("---NO CODE TEST FAILURES---")
    return {"generation": code_solution, "messages": messages, "iterations": iterations, "error": "no"}

def reflect(state: GraphState):
    """
    Reflect on errors

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation
    """

    print("---GENERATING CODE SOLUTION---")

    # State
    messages = state["messages"]
    iterations = state["iterations"]
    code_solution = state["generation"]

    # Prompt reflection
    reflection_message = [("user", """You tried to solve this problem and failed a unit test. Reflect on this failure
                                    given the provided documentation. Write a few key suggestions based on the
                                    documentation to avoid making this mistake again.""")]

    # Add reflection
    reflections = code_gen_chain.invoke({"context"  : concatenated_content, "messages" : messages})
    messages += [("assistant" , f"Here are reflections on the error: {reflections}")]
    return {"generation": code_solution, "messages": messages, "iterations": iterations}

### Edges

def decide_to_finish(state: GraphState):
    """
    Determines whether to finish.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """
    error = state["error"]
    iterations = state["iterations"]

    if error == "no" or iterations == max_iterations:
        print("---DECISION: FINISH---")
        return "end"
    else:
        print("---DECISION: RE-TRY SOLUTION---")
        if flag == 'reflect':
            return "reflect"
        else:
            return "generate"

from langgraph.graph import END, StateGraph

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("generate", generate)  # generation solution
workflow.add_node("check_code", code_check)  # check code
workflow.add_node("reflect", reflect)  # reflect

# Build graph
workflow.set_entry_point("generate")
workflow.add_edge("generate", "check_code")
workflow.add_conditional_edges(
    "check_code",
    decide_to_finish,
    {
        "end": END,
        "reflect": "reflect",
        "generate": "generate",
    },
)
workflow.add_edge("reflect", "generate")
app = workflow.compile()

question = "Draw a red circle"
app.invoke({"messages":[("user",question)],"iterations":0})
# Output:
#   ---GENERATING CODE SOLUTION---

#   ---CHECKING CODE---

#   ---CODE IMPORT CHECK: FAILED---

#   ---DECISION: RE-TRY SOLUTION---

#   ---GENERATING CODE SOLUTION---

#   ---CHECKING CODE---

#   ---CODE IMPORT CHECK: FAILED---

#   ---DECISION: RE-TRY SOLUTION---

#   ---GENERATING CODE SOLUTION---

#   ---CHECKING CODE---

#   ---CODE IMPORT CHECK: FAILED---

#   ---DECISION: FINISH---

#   {'error': 'yes',

#    'messages': [('user', 'Draw a red circle'),

#     ('assistant',

#      'Draw a red circle using Manim \n Imports: from manim import * \n Code: class RedCircle(Scene):\n    def construct(self):\n        circle = Circle(color=RED)\n        self.add(circle)\n        self.wait(2)'),

#     ('user', "Your solution failed the import test: No module named 'manim'"),

#     ('user',

#      'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),

#     ('assistant',

#      'To draw a red circle using Manim, you can use the following code snippet. This example creates a simple scene with a red circle and displays it for a short duration. \n Imports: from manim import * \n Code: class RedCircle(Scene):\n    def construct(self):\n        circle = Circle(color=RED)\n        self.add(circle)\n        self.wait(2)'),

#     ('user', "Your solution failed the import test: No module named 'manim'"),

#     ('user',

#      'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),

#     ('assistant',

#      'To draw a red circle using Manim, follow this structured approach: \n Imports: from manim import * \n Code: class RedCircle(Scene):\n    def construct(self):\n        circle = Circle(color=RED)\n        self.add(circle)\n        self.wait(2)'),

#     ('user', "Your solution failed the import test: No module named 'manim'")],

#    'generation': code(prefix='To draw a red circle using Manim, follow this structured approach:', imports='from manim import *', code='class RedCircle(Scene):\n    def construct(self):\n        circle = Circle(color=RED)\n        self.add(circle)\n        self.wait(2)', description='Schema for code solutions to questions about Manim.'),

#    'iterations': 3}



================================================
FILE: experiments/generative_manim_langgraph_implementation.py
================================================
# -*- coding: utf-8 -*-
"""Generative Manim LangGraph Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YSO9TG2fJVVH4l7yTHE_V-v8VaDfpM2s

# Generative Manim LangGraph Implementation

Taking the example of [Code generation with flow](https://github.com/langchain-ai/langgraph/blob/main/examples/code_assistant/langgraph_code_assistant.ipynb?ref=blog.langchain.dev), we will implement a similar approach to generate code for Manim animations. So far, I think we would not need test validation, we can delay this step for later.
"""

"""## Extracting examples from Manim docs"""
# Load .env
from dotenv import load_dotenv
load_dotenv()

import os
from bs4 import BeautifulSoup as Soup
from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader

# Manim Examples docs
url = "https://docs.manim.community/en/stable/examples.html"
loader = RecursiveUrlLoader(
    url=url, max_depth=20, extractor=lambda x: Soup(x, "html.parser").text
)
docs = loader.load()

# Sort the list based on the URLs and get the text
d_sorted = sorted(docs, key=lambda x: x.metadata["source"])
d_reversed = list(reversed(d_sorted))
concatenated_content = "\n\n\n --- \n\n\n".join(
    [doc.page_content for doc in d_reversed]
)

"""## LLM Solution"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

### OpenAI

# Manim code generation prompt
code_gen_prompt = ChatPromptTemplate.from_messages(
    [("system","""You are a coding assistant with expertise in Manim, Graphical Animation Library. \n
    Here is a full set of Manim documentation:  \n ------- \n  {context} \n ------- \n Answer the user
    question based on the above provided documentation. Ensure any code you provide can be executed \n
    with all required imports and variables defined. Structure your answer with a description of the code solution. \n
    Then list the imports. And finally list the functioning code block. Here is the user question:"""),
    ("placeholder", "{messages}")]
)

class code(BaseModel):
    """Code output"""
    prefix: str = Field(description="Description of the problem and approach")
    imports: str = Field(description="Code block import statements")
    code: str = Field(description="Code block not including import statements")
    description = "Schema for code solutions to requests on code for Manim Animations."


import anthropic

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
anthropic.api_key = os.getenv('ANTHROPIC_API_KEY')

expt_llm = "gpt-4-0125-preview"
llm = ChatOpenAI(temperature=0, model=expt_llm, openai_api_key=OPENAI_API_KEY)
code_gen_chain = code_gen_prompt | llm.with_structured_output(code)

question = "Draw three red circles"

solution = code_gen_chain.invoke({"context":concatenated_content,"messages":[("user",question)]})

solution

"""Now let's try with Anthropic"""

from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

code_gen_prompt_claude = ChatPromptTemplate.from_messages(
    [("system","""<instructions> You are a coding assistant with expertise in Manim, Graphical Animation Library. \n
    Here is a full set of Manim documentation:  \n ------- \n  {context} \n ------- \n Answer the user request based on the \n
    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \n
    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \n
    Invoke the code tool from Manim to structure the output correctly. </instructions> \n Here is the user request:""",),
    ("placeholder", "{messages}"),])

# Data model
class code(BaseModel):
    """Code output"""

    prefix: str = Field(description="Description of the problem and approach")
    imports: str = Field(description="Code block import statements")
    code: str = Field(description="Code block not including import statements")
    description = "Schema for code solutions to questions about Manim."

# LLM
# expt_llm = "claude-3-haiku-20240307"
expt_llm = "claude-3-opus-20240229"
llm = ChatAnthropic(
    model=expt_llm,
    default_headers={"anthropic-beta": "tools-2024-04-04"},
)

structured_llm_claude = llm.with_structured_output(code, include_raw=True)

code_chain_claude_raw = code_gen_prompt_claude | structured_llm_claude

def insert_errors(inputs):
    """Insert errors for tool parsing in the messages"""

    # Get errors
    error = inputs["error"]
    messages = inputs["messages"]
    messages += [
        (
            "assistant",
            f"Retry. You are required to fix the parsing errors: {error} \n\n You must invoke the provided tool.",
        )
    ]
    return {
        "messages": messages,
        "context": inputs["context"],
    }

# This will be run as a fallback chain
fallback_chain = insert_errors | code_chain_claude_raw
N = 3  # Max re-tries
code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(fallbacks=[fallback_chain] * N, exception_key="error")

def parse_output(solution):
    """When we add 'include_raw=True' to structured output,
       it will return a dict w 'raw', 'parsed', 'parsing_error'. """

    return solution['parsed']

# Wtih re-try to correct for failure to invoke tool
# TODO: Annoying errors w/ "user" vs "assistant"
# Roles must alternate between "user" and "assistant", but found multiple "user" roles in a row
code_gen_chain = code_gen_chain_re_try | parse_output

# No re-try
code_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output

# Test
question = "Draw a red circle"
solution = code_gen_chain.invoke({"context":concatenated_content,"messages":[("user",question)]})
solution

"""## State

Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation.
"""

from typing import Dict, TypedDict, List

class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        error : Binary flag for control flow to indicate whether test error was tripped
        messages : With user question, error messages, reasoning
        generation : Code solution
        iterations : Number of tries
    """

    error : str
    messages : List
    generation : str
    iterations : int

"""## Graph

Our graph lays out the logical flow shown in the figure above.


"""

from operator import itemgetter
from langchain.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import RunnablePassthrough

### Parameter

# Max tries
max_iterations = 3
# Reflect
# flag = 'reflect'
flag = 'do not reflect'

### Nodes

def generate(state: GraphState):
    """
    Generate a code solution

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation
    """

    print("---GENERATING CODE SOLUTION---")

    # State
    messages = state["messages"]
    iterations = state["iterations"]
    error = state["error"]

    # We have been routed back to generation with an error
    if error == "yes":
        messages += [("user","Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:")]

    # Solution
    code_solution = code_gen_chain.invoke({"context": concatenated_content, "messages" : messages})
    messages += [("assistant",f"{code_solution.prefix} \n Imports: {code_solution.imports} \n Code: {code_solution.code}")]

    # Increment
    iterations = iterations + 1
    return {"generation": code_solution, "messages": messages, "iterations": iterations}

def code_check(state: GraphState):
    """
    Check code

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, error
    """

    print("---CHECKING CODE---")

    # State
    messages = state["messages"]
    code_solution = state["generation"]
    iterations = state["iterations"]

    # Get solution components
    prefix = code_solution.prefix
    imports = code_solution.imports
    code = code_solution.code

    # Check imports
    try:
        exec(imports)
    except Exception as e:
        print("---CODE IMPORT CHECK: FAILED---")
        error_message = [("user", f"Your solution failed the import test: {e}")]
        messages += error_message
        return {"generation": code_solution, "messages": messages, "iterations": iterations, "error": "yes"}

    # Check execution
    try:
        exec(imports + "\n" + code)
    except Exception as e:
        print("---CODE BLOCK CHECK: FAILED---")
        error_message = [("user", f"Your solution failed the code execution test: {e}")]
        messages += error_message
        return {"generation": code_solution, "messages": messages, "iterations": iterations, "error": "yes"}

    # No errors
    print("---NO CODE TEST FAILURES---")
    return {"generation": code_solution, "messages": messages, "iterations": iterations, "error": "no"}

def reflect(state: GraphState):
    """
    Reflect on errors

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation
    """

    print("---GENERATING CODE SOLUTION---")

    # State
    messages = state["messages"]
    iterations = state["iterations"]
    code_solution = state["generation"]

    # Prompt reflection
    reflection_message = [("user", """You tried to solve this problem and failed a unit test. Reflect on this failure
                                    given the provided documentation. Write a few key suggestions based on the
                                    documentation to avoid making this mistake again.""")]

    # Add reflection
    reflections = code_gen_chain.invoke({"context"  : concatenated_content, "messages" : messages})
    messages += [("assistant" , f"Here are reflections on the error: {reflections}")]
    return {"generation": code_solution, "messages": messages, "iterations": iterations}

### Edges

def decide_to_finish(state: GraphState):
    """
    Determines whether to finish.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """
    error = state["error"]
    iterations = state["iterations"]

    if error == "no" or iterations == max_iterations:
        print("---DECISION: FINISH---")
        return "end"
    else:
        print("---DECISION: RE-TRY SOLUTION---")
        if flag == 'reflect':
            return "reflect"
        else:
            return "generate"

from langgraph.graph import END, StateGraph

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("generate", generate)  # generation solution
workflow.add_node("check_code", code_check)  # check code
workflow.add_node("reflect", reflect)  # reflect

# Build graph
workflow.set_entry_point("generate")
workflow.add_edge("generate", "check_code")
workflow.add_conditional_edges(
    "check_code",
    decide_to_finish,
    {
        "end": END,
        "reflect": "reflect",
        "generate": "generate",
    },
)
workflow.add_edge("reflect", "generate")
app = workflow.compile()

question = "Draw a red circle"
app.invoke({"messages":[("user",question)],"iterations":0})


================================================
FILE: experiments/langgraph_minimal_implementation.py
================================================
# -*- coding: utf-8 -*-
"""Generative Manim LangGraph Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YSO9TG2fJVVH4l7yTHE_V-v8VaDfpM2s

# Generative Manim LangGraph Implementation

Taking the example of [Code generation with flow](https://github.com/langchain-ai/langgraph/blob/main/examples/code_assistant/langgraph_code_assistant.ipynb?ref=blog.langchain.dev), we will implement a similar approach to generate code for Manim animations. So far, I think we would not need test validation, we can delay this step for later.
"""

"""## Extracting examples from Manim docs"""
# Load .env
from dotenv import load_dotenv
load_dotenv()

import os
from bs4 import BeautifulSoup as Soup
from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader

# Manim Examples docs
url = "https://docs.manim.community/en/stable/examples.html"
loader = RecursiveUrlLoader(
    url=url, max_depth=20, extractor=lambda x: Soup(x, "html.parser").text
)
docs = loader.load()

# Sort the list based on the URLs and get the text
d_sorted = sorted(docs, key=lambda x: x.metadata["source"])
d_reversed = list(reversed(d_sorted))
concatenated_content = "\n\n\n --- \n\n\n".join(
    [doc.page_content for doc in d_reversed]
)

"""## LLM Solution"""

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
import os

# Initial configuration
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
llm = ChatOpenAI(temperature=0, model="gpt-4-0125-preview", openai_api_key=OPENAI_API_KEY)

# Message template for the LLM
code_gen_prompt = ChatPromptTemplate.from_messages(
    [("system", "You are a coding assistant with expertise in Manim, the Graphical Animation Library. Please generate code based on the user's request."),
     ("placeholder", "{messages}")]
)

# Data model for structured output
class Code(BaseModel):
    prefix: str = Field(description="Description of the problem and approach")
    imports: str = Field(description="Code block import statements")
    code: str = Field(description="Executable code block")

# Function to handle errors
def handle_errors(inputs):
    error = inputs["error"]
    messages = inputs["messages"]
    messages += [
        ("assistant", f"Please correct the following error: {error}")
    ]
    return {"messages": messages, "context": inputs["context"]}

# Fallback chain for error handling
fallback_chain = handle_errors | (code_gen_prompt | llm.with_structured_output(Code))

# Retry configuration
N = 3  # Number of retries
code_gen_chain_with_retry = (code_gen_prompt | llm.with_structured_output(Code)).with_fallbacks(fallbacks=[fallback_chain] * N, exception_key="error")

# Function to parse the output
def parse_output(solution: Code):
    # Ensure the solution is an instance of Code and access its attributes directly
    if isinstance(solution, Code):
        return {
            "prefix": solution.prefix,
            "imports": solution.imports,
            "code": solution.code
        }
    else:
        raise TypeError("Expected a Code instance")

# Final chain with retries and parsing
code_gen_chain = code_gen_chain_with_retry | parse_output

# Using the chain to generate code
question = "Draw three red circles"
solution = code_gen_chain.invoke({"context": concatenated_content, "messages": [("user", question)]})
print(solution)



================================================
FILE: streamlit/README.md
================================================
# Generative Manim Streamlit (Deprecated)

**Generative Manim** began as an experiment whose demo interface was in [Streamlit](https://streamlit.app/). Here you can find the source code that was used in that demo. The new demo is based on a web app with Next.js and you can find its source code at [360macky/generative-manim-demo](https://github.com/360macky/generative-manim-demo).

## Legacy Documentation

Generative Manim is built with [Streamlit](https://streamlit.io). It uses [OpenAI API](https://platform.openai.com/docs/api-reference/introduction) to make requests to GPT-4 and GPT-3.5-turbo.

![Blueprint](./src/pages/blueprint.png)

For more details, check out the [How it works](https://generative-manim.streamlit.app/%EF%B8%8F_How_it_works) section.



================================================
FILE: streamlit/docker-compose.yml
================================================
version: '3'

services: 
  manim:
    build: 
      context: .
      dockerfile: ./Dockerfile
    image: streamlit-manim
    ports: 
    - 8000:8501
    volumes:
      - ./src:/app/src


================================================
FILE: streamlit/Dockerfile
================================================
FROM python:3.8.0

WORKDIR /app

RUN apt update
RUN apt -y upgrade
RUN apt install -y libcairo2-dev
RUN apt install -y ffmpeg
RUN apt install -y texlive texlive-latex-extra texlive-fonts-extra texlive-latex-recommended texlive-science texlive-fonts-extra
RUN apt install -y libpango1.0-dev pkg-config

RUN pip install poetry

COPY pyproject.toml /app/pyproject.toml

RUN poetry install

COPY src /app/src

CMD ["poetry", "run", "streamlit", "run", "/app/src/Generator.py"]


================================================
FILE: streamlit/src/main.py
================================================
import os
import subprocess
import streamlit as st
from manim import *
from PIL import Image

from utils import *

icon = Image.open(os.path.dirname(__file__) + '/icon.png')

st.set_page_config(
    page_title="Generative Manim",
    page_icon=icon,
)

styl = f"""
<style>
  textarea[aria-label="Code generated: "] {{
    font-family: 'Consolas', monospace !important;
  }}
  }}
</style>
"""
st.markdown(styl, unsafe_allow_html=True)

st.title(":art: Generative Manim")
st.write(":robot_face: Create beautiful and quick animations with GPT-4 and GPT-3.5 :sparkles:")

st.write("## Hello! The new demo it's on [GM Demo](https://generative-manim.vercel.app) :rocket:")

st.write('Made with :heart: by [Marcelo](https://github.com/360macky).')
st.write('[Source code](https://github.com/360macky/generative-manim) - [Report a bug](https://github.com/360macky/generative-manim/issues/new) - [Twitter](https://twitter.com/360macky) - [OpenAI Profile](https://community.openai.com/u/360macky/summary)')



================================================
FILE: streamlit/src/utils.py
================================================
import re

GPT_SYSTEM_INSTRUCTIONS = """Write Manim scripts for animations in Python. Generate code, not text. Never explain code. Never add functions. Never add comments. Never infinte loops. Never use other library than Manim/math. Only complete the code block. Use variables with length of maximum 2 characters. At the end use 'self.play'.

```
from manim import *
from math import *

class GenScene(Scene):
    def construct(self):
        # Write here
```"""

def wrap_prompt(prompt: str) -> str:
  """
    Wraps the prompt in the GPT-3.5 instructions
  """
  return f"Animation Request: {prompt}"

def extract_code(text: str) -> str:
  """
    Extracts the code from the text generated by GPT-3.5 from the ``` ``` blocks
  """
  pattern = re.compile(r"```(.*?)```", re.DOTALL)
  match = pattern.search(text)
  if match:
    return match.group(1).strip()
  else:
    return text

def extract_construct_code(code_str: str) -> str:
  """
    Extracts the code from the construct method
  """
  pattern = r"def construct\(self\):([\s\S]*)"
  match = re.search(pattern, code_str)
  if match:
    return match.group(1)
  else:
    return ""

def code_static_corrector(code_response: str) -> str:
  """
    Corrects some static errors in the code
    GPT only has information until 2021, so it ocasionally generates code
    that is not compatible with the latest version of Manim
  """
  # Replace ShowCreation with Create
  # Solution: https://www.reddit.com/r/manim/comments/qcosuj/nameerror_name_showcreation_is_not_defined/
  code_response = code_response.replace("ShowCreation", "Create")

  return code_response

def create_file_content(code_response: str) -> str:
  """
    Creates the content of the file to be written
  """
  return f"""# Manim code generated with OpenAI GPT
# Command to generate animation: manim GenScene.py GenScene --format=mp4 --media_dir . --custom_folders video_dir

from manim import *
from math import *

class GenScene(Scene):
    def construct(self):
{code_static_corrector(code_response)}"""



================================================
FILE: streamlit/src/pages/1_ğŸ’»_Render_Engine.py
================================================
import streamlit as st
from manim import *
import os
import base64
from PIL import Image

from utils import *

icon = Image.open(os.path.dirname(__file__) + '/../icon.png')

st.set_page_config(page_icon=icon)

styl = f"""
<style>
  textarea {{
    font-family: 'Consolas', monospace !important;
  }}
  }}
</style>
"""
st.markdown(styl, unsafe_allow_html=True)

st.title('ğŸ’» Render Engine')

st.write("Quick engine to render Manim code. Paste your code, and click generate. That's it!")

st.write("## Hello! The new demo it's on [GM Demo](https://generative-manim.vercel.app) :rocket:")



================================================
FILE: streamlit/src/pages/2_ğŸ¤–_Prompt_Engine.py
================================================
import streamlit as st
import os
import openai
from PIL import Image

from utils import *

icon = Image.open(os.path.dirname(__file__) + '/../icon.png')

st.set_page_config(page_icon=icon)

st.markdown('# ğŸ¤– Prompt Engine')

st.write("Prompt engineering is about giving correct instructions to GPT-4. The more precise the instructions, the better the results. The goal is to generate Manim code from a specific part of code. Than you can use the code to render the animation.")

st.write("## Hello! The new demo it's on [GM Demo](https://generative-manim.vercel.app) :rocket:")



================================================
FILE: streamlit/src/pages/3_âš¡ï¸_How_it_works.py
================================================
import streamlit as st
import os
from PIL import Image

icon = Image.open(os.path.dirname(__file__) + '/../icon.png')

st.set_page_config(page_icon=icon)

st.title("âš¡ï¸ How it works")

st.markdown("""
**What do you think about Generative Manim so far?**

Please tell me at [Twitter](https://twitter.com/360macky) or open an issue at the [GitHub Repository](https://github.com/360macky/generative-manim).
""")

st.write("## Concept")

st.markdown("""

Imagine a future where you can watch an animation video from a concept you have in your mind in a few seconds. That would be useful for teachers, students, and more. People that want to generate a video won't need to learn how to use a video editor, how to draw, or how to code. You just need to write a descriptive text.

That indeed is a future. And it's not that far. We can already start working on it.

Manim is a Python library for creating complex graphics and animations. The main advantage of Manim for GPT, it's that since it's a language for the LLM, it's more easy to generate proper code from a prompt.

I have a feel that a pure AI editor like [Runway](https://runwayml.com), or an advanced animation editor like [Jitter](https://jitter.video) could take advantage of a workflow like this:

""")

blueprint = Image.open(os.path.dirname(__file__) + "/blueprint.png")

st.image(blueprint, caption="Blueprint of Generative Manim", output_format="PNG")

st.markdown("""

The idea behind *Generative Manim* is test how far we can go with GPT-3.5 and GPT-4.

## Rendering problems

While GPT-3.5 and GPT-4 are able to generate good code. They have the following limitations:

- The information is limited up to 2021. That means Manim features that were added after 2021 won't be available.
- The code generated by GPT-3.5 and GPT-4 is not always correct.

:white_check_mark: At this moment, **Generative Manim** is able to render simple animations that rely on Manim and Math packages.

:x: But it's not able to render really complex animations that could require more lines of code that the limit of tokens provided by GPT-3.5 or GPT-4.

## Costs implications

Currently GPT-4 is 30 times more expensive than GPT-3.5.

[GPT-4 costs 3 (input) to 6 (generated) cents for 1000 tokens, which is around 1 million tokens for $30](https://github.com/360macky/generative-manim/issues/2). That's a lot of money. But it's not that much if you think about the possibilities.

I would recommend you to use your own API Key if you want to use this app without character limits.

""")


st.write("## Acknowledgements")

st.markdown("""

- [Ashish Shukla](https://github.com/treuille/streamlit-manim/issues/1#issuecomment-1475134874) - For providing the Docker to run Manim in Streamlit.
- [Manim Reddit Community](https://www.reddit.com/r/manim/) - The community of Manim developers.
- [OpenAI Community](https://community.openai.com) - The community of OpenAI developers.
- [Machine Learning Street Talk](https://twitter.com/MLStreetTalk/status/1636647985621745664) - A tweet that inspired the concept for the GPT-4 application.

""")



================================================
FILE: streamlit/src/pages/4_â“_FAQ.py
================================================
import streamlit as st
import os
from PIL import Image

icon = Image.open(os.path.dirname(__file__) + '/../icon.png')

st.set_page_config(page_icon=icon)

st.title("â“ FAQ")

st.markdown("""
If you have any other questions, please tell me at [Twitter](https://twitter.com/360macky)! I'll be happy to answer them.
""")

st.write("## How the future could be?")

st.write("Imagine a video editor that allows you to create animations just by writing a text. Manim could be that way the videos are created.")

st.write("## How can I modify this app?")

st.markdown("""
If you want to experiment over this concept you can fork the [GitHub Repository](https://github.com/360macky/generative-manim), then create an account at [Streamlit](https://streamlit.io/), open the [Dashboard](https://share.streamlit.io/), click on the *New App* button, and then click on the *GitHub* button. Then you can deploy the app to Streamlit, then search for the forked repository, in Main file path write `src/main.py` and finally click on the *Deploy!* button.

Now you can update the cloned repository and view the changes in your Streamlit app. If you believe that your changes are useful, you can open a pull request to the original repository. Pull requests are warmly welcome.
""")

st.write("## What is Manim?")

st.write("Manim is a Python library for creating mathematical animations. It's open source. The version used in this Streamlit app is Manim Community, a fork of the original Manim library.")

st.write("## What is GPT-3.5?")

st.write("GPT-3.5 is the latest stable API-available version of OpenAI's GPT model. It's a language model behind ChatGPT.")

st.write("## What is GPT-4?")

st.write("GPT-4 is the latest (but unstable and limited) version of OpenAI's GPT model. GPT-4 excels at tasks that require advanced reasoning, complex instruction understanding, and more creativity.")

st.write("## What is the purpose of this app?")

st.write("The purpose is to show how GPT-4 and GPT-3.5 can be used to export Manim code from a prompt. The design, which is simple, could be used to empower video editors, or apps in general.")




================================================
FILE: .devcontainer/devcontainer.json
================================================
{
  "name": "Python 3",
  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile
  "image": "mcr.microsoft.com/devcontainers/python:1-3.11-bullseye",
  "customizations": {
    "codespaces": {
      "openFiles": [
        "README.md",
        "src/main.py"
      ]
    },
    "vscode": {
      "settings": {},
      "extensions": [
        "ms-python.python",
        "ms-python.vscode-pylance"
      ]
    }
  },
  "updateContentCommand": "[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo 'âœ… Packages installed and Requirements met'",
  "postAttachCommand": {
    "server": "streamlit run src/main.py --server.enableCORS false --server.enableXsrfProtection false"
  },
  "portsAttributes": {
    "8501": {
      "label": "Application",
      "onAutoForward": "openPreview"
    }
  },
  "forwardPorts": [
    8501
  ]
}

